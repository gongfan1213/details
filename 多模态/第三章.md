
https://slds-lmu.github.io/seminar_multimodal_dl/c02-00-multimodal.html

# Chapter 3 Multimodal architectures 网页总结
## 一、章节概述
本章聚焦多模态深度学习中的图像与文本模态融合架构，探讨了图像到文本（Image2Text）、文本到图像（Text2Image）、图像支持语言模型、文本支持视觉模型以及双模态通用模型五大核心方向。通过分析主流模型架构、数据集、训练方法及性能对比，揭示了多模态融合的技术突破与挑战，旨在为读者呈现多模态深度学习在图像-文本交互领域的技术全貌与研究趋势。


## 二、核心章节内容
### 3.1 Image2Text（图像到文本）
#### 3.1.1 关键数据集：Microsoft COCO
- **数据集定位**：2014年发布的大型图像数据集，全称为“Common Objects in Context”，核心目标是推进场景理解（目标检测、分割、图像描述）的技术水平。
- **数据规模**：包含91个常见物体类别，32.8万张图像，250万个实例标注；82个类别拥有超过5000个标注实例，仅收录“有明确边界的物体”（如汽车、桌子），排除无明确边界的类别（如天空、草地）。
- **数据特点**：以“非标志性图像”（含多个物体、日常场景）为主，平均每张图像含3.5个类别、7.7个实例，提供丰富的上下文信息。
- **与其他数据集对比**：
  | 数据集 | 核心任务 | 类别数 | 平均每图实例数 | 特点 |
  |--------|----------|--------|----------------|------|
  | MS COCO | 目标检测、分割 | 91 | 7.7 | 非标志性图像为主，上下文丰富 |
  | ImageNet | 图像分类 | 22k | <3 | 类别多（含细粒度类别），图像规模超1400万张 |
  | PASCAL VOC | 目标检测 | 20 | <3 | 含27万个标注实例，支持基础目标检测 |
  | SUN | 场景理解 | 908（场景） | 17 | 结合目标检测与场景标注，含3819个物体类别 |

#### 3.1.2 图像描述模型演进
- **早期模型**：基于模板生成（如利用目标检测器输出构建描述）、RNN/LSTM（结合CNN提取视觉特征，实现序列生成），但存在“表示能力有限”“依赖序列处理”的缺陷。
- **Transformer-based模型**：采用Transformer编码器（处理图像特征）+解码器（生成文本），代表模型包括：
  - **Meshed-Memory Transformer（M²）**：2020年提出的全注意力模型，核心创新为“多级别图像关系编码”（融合低/高级视觉关系）和“持久化记忆向量”（学习先验知识），在MS COCO数据集上的CIDEr评分达131.2，超越传统Transformer（121.8）及简化版M²（129.2）。

#### 3.1.3 模型性能评估
- **评估指标**：采用BLEU、METEOR、ROUGE、CIDEr、SPICE等标准指标。
- **M²优势**：在MS COCO“Karpathy分割”测试集上，BLEU-4、METEOR、CIDEr指标超越SCST、Up-Down、GCN-LSTM等主流模型，能生成更精准的图像描述（如捕捉物体间细节关系）。


### 3.2 Text2Image（文本到图像）
#### 3.2.1 模型评估指标
- **Inception Score（IS）**：基于Inception Net分类生成图像，衡量“类别多样性”（p(y)熵）与“图像清晰度”（p(y|x)确定性），分数越高越好。
- **Fréchet Inception Distance（FID）**：计算真实图像与生成图像在Inception Net特征空间的Fréchet距离，距离越小表示生成质量越接近真实。
- **Precision/Recall**：衡量生成图像分布（P₉）对真实分布（Pᵣ）的“覆盖度”（Recall）与“真实性”（Precision），弥补IS/FID的单维度缺陷。
- **CLIP Score**：通过CLIP模型计算文本与生成图像的语义相似度，反映“文本-图像对齐度”。
- **人类评估**：从“照片真实度”“文本-图像相似度”两个维度对生成结果打分。

#### 3.2.2 主流模型演进
- **生成对抗网络（GAN）系列**：
  - **GAN-INT-CLS（2016）**：首个结合文本嵌入的GAN，通过“文本编码+噪声向量”驱动生成器，支持零样本生成（如未见过的鸟类类别）。
  - **StackGAN（2016）**：采用“两阶段生成”，Stage-I生成低分辨率图像，Stage-II优化细节与分辨率（64x64→256x256）。
  - **AttnGAN（2017）**：引入注意力机制，生成器各阶段关注文本关键词，支持注意力可视化。
  - **DF-GAN（2020）**：单阶段生成架构，结合 hinge 损失与残差块，提升语义一致性与收敛效率。
- **VAE/VQ-VAE系列**：
  - **DALL-E 1（2021）**：基于dVAE（离散变分自编码器）压缩图像为32x32令牌，结合Transformer autoregressive生成；使用2.5亿文本-图像对训练，零样本能力超越传统模型，但生成真实度较低。
- **扩散模型（Diffusion）系列**：
  - **GLIDE（2021）**：结合文本引导的扩散模型，采用U-Net架构+CLIP文本嵌入，通过“无分类器引导”（缩放文本条件与非条件生成的差异）提升生成质量；在MS COCO 256x256数据集上FID显著低于DALL-E 1，人类偏好率超90%，但复杂文本 prompt 处理能力弱、推理速度慢。
  - **DALL-E 2（UnCLIP，2022）**：由“Prior（生成CLIP图像嵌入）”和“Decoder（生成图像）”组成，基于GLIDE优化，支持1024x1024分辨率生成，但存在“物体位置混淆”“文本生成错误”等问题。
- ** autoregressive 模型**：
  - **Parti（2022）**：采用ViT-VQGAN令牌化图像，结合Transformer autoregressive生成；在MS COCO数据集上FID最优，但仍存在“特征混合错误”“计数能力弱”等缺陷。

#### 3.2.3 行业趋势与伦理挑战
- **开源vs闭源**：OpenAI（DALL-E系列）、Google（Imagen/Parti）等倾向闭源，而社区开源模型（Stable Diffusion、Latent Diffusion）普及度高。
- **应用场景**：图形设计、影视制作、产品设计、虚拟现实场景生成等。
- **伦理问题**：虚假图像生成（造谣、骚扰）、训练数据偏见（如性别/种族刻板印象）、版权归属争议（生成内容的作者认定）。


### 3.3 Images supporting Language Models（图像支持语言模型）
#### 3.3.1 核心动机：符号接地问题（Symbol Grounding Problem）
传统语言模型（如Word2Vec、BERT）依赖文本上下文学习语义，但无法“接地”到非符号感知空间（如图像），导致抽象词理解不足。图像支持语言模型通过融合视觉特征，提升语义表示的准确性。

#### 3.3.2 模型演进
- **早期融合（串联/并行学习）**：
  - **Bruni et al.（2014）**：串联文本语义矩阵与图像语义矩阵，通过SVD投影到多模态空间。
  - **Silberer & Lapata（2014）**：文本与视觉特征分别通过单模态自编码器，再输入双模态自编码器融合。
- **接地空间（Grounded Space）**：
  - **Lazaridou et al.（2015）**：扩展Skip-gram模型，加入视觉损失（使文本嵌入与视觉嵌入余弦相似度最大化），支持“无图像词的视觉表示生成”。
  - **Bordes et al.（2020）**：将文本与视觉特征投影到“接地空间”，通过中间空间训练，避免文本-视觉一对一映射限制。
- **Transformer时代**：
  - **Vokenization（Tan & Bansal，2020）**：为每个文本令牌分配“视觉令牌（Voken）”，通过对比损失学习文本-视觉关联；解决“接地率低”问题（纯文本语料中仅1/3令牌有视觉对应），在GLUE等基准上提升纯语言模型性能。
  - **iACE（Lu et al.，2022）**：利用VQGAN+CLIP生成文本的“想象图像”，通过跨模态编码器融合文本与想象图像特征，在少样本场景（如仅用5%训练数据）下性能优于传统视觉监督模型。

#### 3.3.3 模型评估
- **预Transformer时代**：关注“词嵌入与人类相似度判断的相关性”，如在MEN、WordSim353数据集上，图像支持模型对“具体词”（如猫、桌子）的表示优于纯语言模型，但对“抽象词”（如爱、和平）无提升。
- **Post-Transformer时代**：聚焦下游任务性能（GLUE、SQuAD、SWAG），纯语言模型（如BERT、RoBERTa）仍占优，但“视觉监督预训练”（如Voken分类任务）可提升模型泛化能力；少样本学习成为关键评估维度，iACE等模型在低数据量下表现突出。


### 3.4 Text supporting Vision Models（文本支持视觉模型）
#### 3.4.1 核心概念
- **Web-scale数据**：利用互联网文本-图像对（无需人工标注）训练，代表数据集规模：CLIP（4亿对）、Florence（9亿对）、ALIGN（18亿对）；需过滤噪声（如非英文文本、无意义alt-text），但仍存在数据偏见问题。
- **对比学习目标（Contrastive Objective）**：联合训练文本编码器与图像编码器，通过最小化对比损失，使“匹配文本-图像对”的嵌入相似度最大化，“非匹配对”最小化；数据效率优于生成式目标（如生成图像描述），CLIP的对比目标数据效率为生成式的7倍。
- **基础模型与零样本学习（Zero-Shot）**：预训练模型无需下游任务微调即可迁移，如CLIP通过“文本编码类别标签+图像编码待分类图像”实现零样本分类，可应用于光学字符识别、地理定位等任务。

#### 3.4.2 主流模型
- **CLIP（2021）**：
  - **架构**：文本编码器（基于GPT-2的Transformer）、图像编码器（ResNet/ViT）；最佳版本ViT-L/14@336px，在ImageNet零样本准确率达76.2%，对分布偏移的鲁棒性显著优于ResNet101（如在ObjectNet上准确率32.6% vs 低准确率）。
  - **应用**：作为视觉编码器集成到V&L模型（如图像描述），但ViT backbone的视觉定位能力弱于ResNet。
- **ALIGN（2021）**：
  - **特点**：减少数据清洗，使用18亿噪声文本-图像对；文本编码器基于BERT-Large，图像编码器基于EfficientNet-L2，参数约8亿，性能与CLIP相当。
- **Florence（2021）**：
  - **架构**：分层ViT（CoSwin Transformer）图像编码器、CLIP风格文本编码器；支持“模块扩展”（如细粒度检测、动作识别），在ImageNet零样本准确率达83.5%，超越CLIP和ALIGN。

#### 3.4.3 性能对比
| 模型 | ImageNet零样本准确率 | MS COCO零样本图像检索R@1 | 优势场景 |
|------|----------------------|--------------------------|----------|
| CLIP（ViT-L/14@336px） | 76.2% | ～50% | 鲁棒性强，开源可访问 |
| ALIGN | ～75% | ～52% | 噪声数据处理能力强 |
| Florence | 83.5% | ～58% | 多任务扩展能力，性能最优 |


### 3.5 Models for both modalities（双模态通用模型）
#### 3.5.1 代表性模型
- **Data2vec（2022，Meta）**：
  - **特点**：自监督单模态模型，支持文本、图像、语音，但不支持跨模态交互；采用“教师-学生架构”，教师模型基于完整输入生成上下文 latent 表示，学生模型基于掩码输入预测该表示。
  - **性能**：在ImageNet分类（ViT-L模型top1准确率超MAE、DINO）、GLUE基准（平均性能超RoBERTa）上达SOTA，需模态特定的编码/掩码策略（如图像用ViT掩码、文本用BERT掩码）。
- **VilBert（2019）**：
  - **架构**：双流Transformer（文本流+图像流），通过“共注意力层”（Cross-Attention）实现模态交互；文本流基于BERT，图像流基于Faster R-CNN提取区域特征+空间位置编码。
  - **预训练任务**：掩码多模态建模（掩码文本/图像令牌并重建）、多模态对齐预测（判断文本-图像是否匹配）；在VQA、图像检索等任务上，双流架构性能优于单流架构。
- **Flamingo（2022，Google）**：
  - **特点**：少样本视觉-语言模型，参数80亿（核心为70亿参数的Chinchilla LLM）；采用“冻结预训练模型+可学习模块”架构：冻结视觉编码器（CLIP风格）与LLM，加入Perceiver-Resampler（将图像特征转为固定长度令牌）和门控跨注意力层（融合视觉与文本特征）。
  - **性能**：在18个视觉-语言基准（如图像描述、视频问答）上超越现有少样本模型，支持开放域文本生成，但分类任务性能弱于对比学习模型（如CLIP）。

#### 3.5.2 关键结论
- **模型规模与性能**：更大模型（如Flamingo）通常性能更优，但需平衡训练成本；
- **训练策略**：冻结预训练模型（避免能力退化）、多数据集加权损失（提升泛化）、少样本学习（降低标注依赖）是核心方向；
- **数据重要性**：异质数据（如Web-scale文本-图像对）提升模型通用性，需关注数据偏见与版权问题。


## 三、总结与展望
本章系统梳理了图像-文本多模态架构的技术演进，从单方向转换（Image2Text/Text2Image）到双向支持（图像辅助语言/文本辅助视觉），再到双模态通用模型，展现了多模态融合从“任务特定”到“通用灵活”的发展趋势。当前挑战包括：抽象概念理解、复杂场景生成、数据偏见与伦理风险；未来方向将聚焦“更高效的跨模态融合”“少样本/零样本泛化”“多模态大模型的轻量化”，并推动技术在实际场景（如内容创作、人机交互）的落地。
