https://slds-lmu.github.io/seminar_multimodal_dl/c01-00-intro-modalities.html#c01-01-sota-nlp

# 《Chapter 2 Introducing the modalities》网页总结
## 一、章节概述
本章聚焦多模态深度学习中的核心模态——自然语言处理（NLP）与计算机视觉（CV），系统梳理了两大领域的前沿技术、关键架构、数据集及基准测试，同时探讨了跨模态任务的基础资源，为后续多模态架构的学习奠定基础。章节作者包括Cem Akkus、Vladana Djakovic、Christopher Benjamin Marquardt，分别负责NLP、CV及资源与基准测试部分的撰写。

## 二、自然语言处理（NLP）前沿技术
### （一）核心突破技术
1. **词嵌入（Word Embeddings）**
    - **背景**：解决传统文本表示（如独热编码、词袋模型）的维度灾难和泛化能力差问题，将单词编码为捕获语义的密集向量。
    - **关键方法**：
        - Word2Vec（Mikolov et al., 2013）：包含连续词袋模型（CBOW，通过上下文预测中心词）和跳字模型（Skip-gram，通过中心词预测上下文），基于浅层神经网络训练。
        - GloVe（Bojanowski et al., 2016）：结合局部上下文与全局共现统计，无需神经网络。
    - **优势**：相似词在低维向量空间中距离更近，支持语义推理（如“King - Man + Woman ≈ Queen”），可应用于跨语言翻译（如英西数字向量映射精度近90%）。
    - **局限**：传统词嵌入为静态表示，无法捕捉一词多义（如“bank”的“银行”与“河岸”含义）。
2. **编码器-解码器（Encoder-Decoder）架构**
    - **用途**：解决输入与输出序列长度不同的任务（如机器翻译、视频字幕生成、问答系统）。
    - **原理**：
        - 编码器将可变长度输入序列转化为固定长度上下文向量（通常为编码器最后一个隐藏状态）。
        - 解码器基于上下文向量生成可变长度输出序列，支持不同语序、主动/被动语态处理。
    - **优化**：采用门控循环网络（如LSTM）提升性能，深层LSTM（如4层结构）比浅层模型 perplexity 降低近10%，通过束搜索（Beam Search）优化输出概率。
3. **注意力机制（Attention）**
    - **解决问题**：突破编码器-解码器的“瓶颈问题”（单一上下文向量压缩全部语义），缓解长序列中的梯度消失。
    - **原理**：生成每个目标词时，模型聚焦源句子中关键信息，计算注意力分数（如点积、加性注意力）并转化为概率分布，加权求和编码器隐藏状态得到注意力输出，与解码器隐藏状态结合生成结果。
    - **效果**：显著提升机器翻译性能，可通过注意力对齐可视化（如英法翻译中形容词与名词的语序调整）解释模型决策。
4. **Transformer架构**
    - **核心创新**：完全基于自注意力机制，无需顺序处理输入（区别于RNN），支持并行计算，更适合长序列上下文记忆，且能高效利用GPU。
    - **关键组件**：
        - 自注意力（Self-Attention）：同一序列内单词相互关注，通过查询（Query）、键（Key）、值（Value）计算注意力权重，捕捉长距离依赖。
        - 缩放点积注意力（Scaled Dot-Product Attention）：将注意力分数除以键维度的平方根，避免数值过大导致softmax梯度消失。
        - 多头注意力（Multi-Head Attention）：多个独立注意力头并行计算，分别捕捉不同类型特征（如时态、主题），再拼接输出。
        - 位置编码（Positional Encoding）：通过正弦/余弦函数注入单词顺序信息，解决自注意力无序列感知的问题。
        - 残差连接与层归一化：缓解深层网络训练困难，防止信息丢失。
    - **优势**：成为当前NLP主流范式，并行效率远超RNN，支持大规模预训练。

### （二）主流Transformer架构
|架构|核心特点|预训练任务|适用场景|
|----|----|----|----|
|BERT（Devlin et al., 2018）|堆叠编码器，双向上下文|掩码语言模型（15%token掩码，80%替换为[MASK]、10%随机替换、10%不变）、下一句预测（后续被弃用）|问答、情感分析、文本摘要（不适合自回归生成）|
|T5（Raffel et al., 2019）|编码器-解码器结合，统一文本到文本格式|跨度损坏（替换词块为占位符并重建）|机器翻译、文档摘要、分类（如情感分析）、回归（预测数字字符串）|
|GPT-3（Brown et al., 2020）|堆叠解码器，自回归生成|语言模型（预测下一个词）|对话生成、代码编写、少样本学习（1750亿参数，预训练数据含5000亿token）|

### （三）当前研究热点
1. **模型规模增长的担忧**：训练大型模型（如Transformer“big”）碳排放达249吨CO₂（是人类年均排放的近50倍），数据质量差、文档不全，且网络数据易强化霸权观点，存在对边缘化群体的偏见。
2. **Transformer可解释性**：BERT注意力权重可反映词性、句法块等层级信息，但难以处理视觉/感知知识（如“房子比人高”的推理），多数注意力头可裁剪而不影响性能。
3. **少样本学习（Few-Shot Learning）**：在少量标注示例（如1-10个）上指导模型预测，GPT-3通过上下文学习提升性能，但受示例顺序、解码策略影响大，真实能力可能被高估。


## 三、计算机视觉（CV）前沿技术
### （一）发展历史
1. **起源（1950s-1960s）**：基于猫的神经生理学研究，提出人类视觉层级结构（从边缘检测到复杂特征），启发人工神经网络模拟；1954年首个Hebbian网络实现，1959年ADALINE/MADALINE模型用于二进制模式识别。
2. **低谷（1969-2010）**：《Perceptrons》指出单层感知器无法解决非线性问题，进入“AI寒冬”，技术发展停滞。
3. **突破（2012年至今）**：AlexNet（Krizhevsky et al., 2012）在ImageNet竞赛中实现16.4%错误率，开启深度CV时代，后续ResNet、EfficientNet、Vision Transformer等架构持续突破。

### （二）学习范式
|学习类型|数据特点|核心任务|代表应用|
|----|----|----|----|
|监督学习|标注数据|分类（图像类别预测）、回归（目标坐标预测）|ImageNet图像分类、COCO目标检测|
|无监督学习|无标注数据|聚类（相似图像分组）、关联（特征关联挖掘）、降维（特征压缩）|图像聚类、异常检测|
|半监督学习|部分标注+大量无标注数据|特征提取|医疗图像分析（标注稀缺场景）|
|自监督学习|无标注，从数据自身生成监督信号|对比学习（区分相似/不同样本）、自预测（重建输入）|SimCLR、BYOL的视觉表示学习|

### （三）核心网络架构
1. **深度残差网络（ResNet, He et al., 2015）**
    - **解决问题**：深层网络的退化问题（层数增加导致训练精度下降，非过拟合）。
    - **核心设计**：
        - 残差学习：将目标映射H(x)转化为残差映射F(x)=H(x)-x，学习F(x)后通过 shortcut 连接（F(x)+x）重建H(x)，降低优化难度。
        - 身份映射与投影 shortcut：输入输出维度相同时用身份映射（无额外参数），维度不同时用1×1卷积投影匹配维度。
    - **性能**：ResNet-152在ImageNet上Top-1准确率达78.57%，远超同期plain-34模型（71.46%）。
2. **高效网络（EfficientNet, Tan & Le, 2019）**
    - **创新点**：提出复合缩放策略，统一缩放网络深度（d）、宽度（w）、图像分辨率（r），而非单一维度缩放。
    - **公式**：d=α^φ，w=β^φ，r=γ^φ（α·β²·γ²≈2，α=1.2、β=1.1、γ=1.15为EfficientNet-B0最优参数，φ控制资源投入）。
    - **性能**：EfficientNet-B7在ImageNet上Top-1准确率达84.3%，与GPipe相当，但参数规模更小（66M vs GPipe的更大规模）。
3. **对比学习框架**
    - **SimCLR（Chen et al., 2020）**：
        - 流程：对图像生成两个增强视图（正样本对），通过编码器提取特征，投影头映射到特征空间，用NT-Xent损失（归一化温度缩放交叉熵）最大化正样本相似度、最小化负样本相似度。
        - 关键发现：组合数据增强（如随机裁剪+颜色失真）、非线性投影头、大批次训练（256-8192）可提升性能，ResNet-50(4x)在ImageNet上Top-1准确率达76.5%。
    - **BYOL（Grill et al., 2020）**：
        - 创新：无需负样本，通过在线网络（预测器）与目标网络（在线网络参数的指数移动平均）交互，预测增强视图的目标表示。
        - 性能：ResNet-50在ImageNet上Top-1准确率达74.3%，对图像增强的依赖性低于SimCLR。
4. **视觉Transformer（ViT, Dosovitskiy et al., 2020）**
    - **原理**：将图像分割为16×16（或14×14） patches，线性投影为token，添加[class]token和位置编码，输入Transformer编码器，通过[class]token输出进行分类。
    - **优势**：归纳偏置低（仅依赖patch分割和位置编码），预训练后在大数据集上性能优异，ViT-Huge在ImageNet上准确率达88.5%，超过ResNet-BiT（87.54%）和Noisy Student（88.4%）。
    - **局限**：在小数据集上性能略逊于CNN，需依赖大规模预训练（如JFT-300M）。


## 四、NLP、CV及多模态任务的资源与基准测试
### （一）关键数据集
1. **NLP数据集**
    - **Common Crawl**：非盈利组织提供的网页爬取数据，含PB级文本，需过滤处理（如GPT-3用其4100亿token子集）。
    - **The Pile**：825GB多源数据，含22个子数据集（如OpenWebText2、ArXiv论文、数学问题集），95%为英文，支持通用语言模型预训练。
    - **BooksCorpus**：11k本电子书（7400万句子），但存在版权问题、 genre 分布不均，目前已停止分发。
    - **多语言数据集**：CC-100（100+语言）、Bible Corpus（900+语言）、mC4（100+语言，6.3万亿token，Google用于mT5预训练）。
2. **CV数据集**
    - **ImageNet**：ImageNet-1k（1000类，120万训练图）为CV预训练基准，ImageNet-21k（2.2万类，1400万图）标签非互斥，缺乏官方划分，使用率较低。
    - **JFT/EFT**：Google私有数据集，JFT-300M含3亿图（1.8万类），EFT含10万类，但无公开文档。
    - **Objects365**：365类目标检测数据集，60万训练图，1000万 bounding boxes，支持语义分割。
    - **COCO**：32.8万图，91类目标，含图像-文本对，用于目标检测、分割及多模态任务。
3. **多模态数据集**
    - **LAION系列**：LAION-400M（4亿图像-文本对，CLIP过滤相似度≥0.3）、LAION-5B（58.5亿对，240TB，当前最大公开多模态数据集）。
    - **Localized Narratives**：为COCO、Flickr30k等数据集添加同步语音-文本-图像标注，支持细粒度跨模态任务。
    - **WuDaoMM**：6.5亿中文图像-文本对，基础版含500万高质量样本，分强关联（专业网站数据）和弱关联数据。
    - **WIT**：3760万图像-文本对，覆盖108种语言，基于Wikipedia编辑机制保证质量，98.5%样本通过人工验证。

### （二）数据偏见问题
- **来源**：互联网访问不均（年轻用户、发达国家用户过度代表）、标注偏差（如WordNet含冒犯性词汇）、爬取数据质量差（如LAION-400M含色情、种族歧视内容）。
- **案例**：GPT-2生成文本存在性别/种族偏见（如“Black man”关联“pimp”，“White man”关联“president”）；TinyImages因含冒犯性图像被下架，MS-Celeb因隐私问题撤回。

### （三）预训练任务
1. **NLP预训练**：掩码语言模型（BERT）、跨度损坏（T5）、自回归语言模型（GPT）。
2. **CV预训练**：对比学习（SimCLR/BYOL）、掩码自编码器（MAE，遮挡75%图像patch并重建）、监督分类（ImageNet预训练）。
3. **多模态预训练**：对比学习（CLIP/ALIGN，最大化图像-文本相似度）、图像-文本匹配（如COCO的 caption 预测）。

### （四）基准测试（Benchmarks）
1. **NLP基准**
    - **GLUE/SuperGLUE**：GLUE含9个任务（如CoLA语法判断、SST-2情感分析），SuperGLUE为更难版本，含8个任务，提供公开排行榜和人类基线。
    - **SQuAD**：SQuAD 1.1含10.8万问答对（答案必在上下文），SQuAD 2.0添加5万无答案问题，评估模型“知不知”能力。
    - **BIG-bench**：200+任务的动态基准，支持用户提交任务，评估模型泛化能力，当前大模型（如GPT-3）性能仍低于人类专家。
    - **WMT**：年度机器翻译竞赛，评估双语/多语翻译性能，用BLEU分数（0-1，0.6-0.7为优）和人工评估排名。
2. **CV基准**
    - **ImageNet系列**：ImageNet-V2检测模型泛化能力，ImageNet-ReaL修正标签偏差，用Top-1/Top-5准确率评估分类性能。
    - **COCO/Objects365**：目标检测用mAP@IoU=0.50:0.05:0.95评估，语义分割用mean IoU评估。
    - **ADE20k**：2万图的语义分割数据集，含273类目标，用mean IoU评估像素级分类性能。
3. **多模态基准**
    - **VCR**：29万电影场景问答对，含“问题→答案→理由”三步任务，人类准确率超90%，当前最佳模型Q→AR准确率74.9%。
    - **VQA**：VQA 2.0平衡数据集（每问题对应两张相似但答案不同的图），解决语言先验偏差（如“yes”盲目回答的高准确率问题）。
    - **GQA**：11.3万图、2200万推理型问题，评估目标识别、空间推理等能力，引入一致性（Consistency）、有效性（Validity）、合理性（Plausibility）指标。
    - **生成模型基准**：用FID（图像质量）、BLEU/CIDEr（文本-图像对齐）和人工评估（如PartiPrompts、DrawBench的prompt测试）比较DALL-E2、Imagen等生成模型。
4. **其他基准**：CheckList（NLP行为测试，含最小功能、不变性、定向期望测试）、VALSE（多模态模型的语言现象敏感性测试）、能源消耗基准（如训练BERT的碳排放相当于跨美航班，Strubell et al., 2019）。


## 五、核心总结
1. **技术趋势**：NLP从词嵌入发展到Transformer（BERT/T5/GPT），CV从CNN（ResNet/EfficientNet）过渡到ViT，多模态任务依赖跨模态数据集（LAION/COCO）和对比学习。
2. **关键挑战**：模型规模增长带来的环境/成本问题、数据偏见与伦理风险、小样本学习泛化能力、多模态模型的可解释性。
3. **资源价值**：公开高质量
