<img width="1300" height="952" alt="image" src="https://github.com/user-attachments/assets/6a459015-f457-4c3a-8cda-8fbf215b80a3" />

<img width="1300" height="952" alt="image" src="https://github.com/user-attachments/assets/8d12b8c2-5d71-40fe-9d02-d8e34d5745a7" />

Co-Learning 协同学习
Multimodal Co-learning aims to transfer information learned through one or more modalities to tasks involving another. Co-learning is especially important in cases of low-resource target tasks, fully/partly missing or noisy modalities. 
多模态协同学习旨在将通过一种或多种模态学到的信息迁移到涉及另一种模态的任务中。在目标任务资源匮乏、模态完全/部分缺失或存在噪声的情况下，协同学习尤为重要。

Translation—explained in the section above—may be used as a method of co-learning to transfer knowledge from one modality to another. Neuroscience suggests that humans may use methods of co-learning through translation, as well. People who suffer from aphantasia, the inability to create mental images in their heads, perform worse on memory tests. The opposite is also true, people who do create such mappings, textual/auditory to visual, perform better on memory tests. This suggests that being able to convert representations between different modalities is an important aspect of human cognition and memory.
上文部分所阐释的翻译可作为一种协同学习方法，用于将知识从一种模态转移到另一种模态。神经科学表明，人类或许也会通过翻译来运用协同学习的方法。患有心象缺失症（即无法在脑海中形成心理图像）的人在记忆测试中的表现更差。反之亦然，那些能建立起文本/听觉与视觉之间此类映射的人，在记忆测试中的表现更好。这表明，能够在不同模态之间转换表征是人类认知和记忆的一个重要方面。


How does Multimodal Learning work 多模态学习是如何运作的
Multimodal neural networks are usually a combination of multiple unimodal neural networks. For example, an audiovisual model might consist of two unimodal networks, one for visual data and one for audio data. These unimodal neural networks usually process their inputs separately. This process is called encoding. After unimodal encoding takes place, the information extracted from each model must be fused together. Multiple fusion techniques have been proposed that range from simple concatenation to attention mechanisms. The process of multimodal data fusion is one of the most important success factors. After fusion takes place, a final “decision” network accepts the fused encoded information and is trained on the end task.
多模态神经网络通常是多个单模态神经网络的组合。例如，一个视听模型可能由两个单模态网络组成，一个用于处理视觉数据，另一个用于处理音频数据。这些单模态神经网络通常会分别处理各自的输入，这个过程称为编码。在完成单模态编码后，从每个模型中提取的信息必须融合在一起。目前已经提出了多种融合技术，从简单的拼接方法到注意力机制不等。多模态数据融合过程是最重要的成功因素之一。融合完成后，一个最终的“决策”网络会接收融合后的编码信息，并针对最终任务进行训练。

To put it simply, multimodal architectures usually consist of three parts:
简单来说，多模态架构通常由三部分组成：

Unimodal encoders that encode individual modalities. Usually, one for each input modality.
单模态编码器，用于对单个模态进行编码。通常，每种输入模态对应一个单模态编码器。

A fusion network that combines the features extracted from each input modality, during the encoding phase.
一种在编码阶段结合从每种输入模态中提取的特征的融合网络。

A classifier that accepts the fused data and makes predictions.
一个接受融合数据并进行预测的分类器。

We refer to the above as the encoding module (DL Module in the image below), fusion module, and classification module. 
我们将上述内容分别称为编码模块（下图中的深度学习模块）、融合模块和分类模块。


Encoding  编码
During encoding, we seek to create meaningful representations. Usually, each individual modality is handled by a different monomodal encoder. However, it’s often the case that the inputs are in the form of embeddings instead of their raw form. For example, word2vec embeddings may be used for text, and COVAREP embeddings for audio. Multimodal embeddings such as data2veq, which translate video, text, and audio data into embeddings in a high dimensional space, are one of the latest practices and have outperformed other embeddings achieving SOTA performance in many tasks.
在编码过程中，我们力求创建有意义的表示。通常，每个单独的模态由不同的单模态编码器处理。然而，输入往往是以嵌入的形式而非原始形式存在。例如，word2vec嵌入可用于文本，COVAREP嵌入可用于音频。像data2veq这样的多模态嵌入是最新的实践之一，它能将视频、文本和音频数据转换为高维空间中的嵌入，并在许多任务中表现优于其他嵌入，达到了SOTA性能。

Deciding whether it's more suitable to use joint representations or coordinated representations (explained in the representation challenge) is an important decision. Usually, a joint representation method works well when modalities are similar in nature, and it’s the one most often used. 
决定是使用联合表示还是协同表示（在表示挑战中已说明）是一个重要的决策。通常，当模态在本质上相似时，联合表示方法效果很好，这也是最常用的方法。

In practice when designing multimodal networks, encoders are chosen based on what works well in each area since more emphasis is given to designing the fusion method. Many research papers use the all-time-classic ResNets for the visual modalities and RoBERTA for text. 
在实际设计多模态网络时，编码器的选择基于其在各个领域的表现，因为更多的重点被放在了融合方法的设计上。许多研究论文在视觉模态中使用经典的ResNets，在文本处理中使用RoBERTA。


Fusion 融合
The fusion module is responsible for combining each individual modality after feature extraction is completed. The method/architecture used for fusion is probably the most important ingredient for success.
融合模块负责在特征提取完成后对各个单独的模态进行组合。用于融合的方法/架构可能是成功的最重要因素。

The simplest method is to use simple operations such as concatenating or summing the different unimodal representations. However, more sophisticated and successful methods have been researched and implemented. For example, the cross-attention layer mechanism is one of the more recent and successful fusion methods. It has been used to capture cross-modal interactions and fuse modalities in a more meaningful way. The equation below describes the cross-attention mechanism and assumes basic familiarity with self-attention.
最简单的方法是使用简单的操作，例如对不同的单模态表示进行拼接或求和。然而，更复杂且更成功的方法已被研究和实现。例如，交叉注意力层机制是较新且较为成功的融合方法之一。它已被用于捕获跨模态交互，并以更有意义的方式融合模态。下面的等式描述了交叉注意力机制，并假设读者对自注意力有基本的了解。

 
‍


Where 
 denotes the attention score vector, 
 denotes the softmax function, 
, 
 and 
 are the Key, Query and Value matrices of the attention mechanism respectively. For symmetry 
 is also computed, and the two may be summed up to create an attention vector that maps the synergy between the two modalities 
 involved.  Essentially, the difference between 
 and 
 is that in the former 
 is used as the query while in the latter 
 is used instead, and 
 takes the role of key and value.
其中，
表示注意力分数向量，
表示softmax函数，
、
和
分别是注意力机制中的键（Key）、查询（Query）和值（Value）矩阵。为了对称性，也会计算
，并且可以将两者相加，生成一个注意力向量，该向量映射所涉及的两种模态
之间的协同作用。本质上，
和
的区别在于，前者中
用作查询，而后者中则改用
，且
承担键和值的角色。
In the case of three or more modalities, multiple cross-attention mechanisms may be used so that every different combination is calculated. For example, if we have vision (V), text (T), and audio (A) modalities, then we create the combinations VT, VA, TA, and AVT in order to capture all possible cross-modal interactions.
在涉及三种或更多模态的情况下，可以使用多种交叉注意力机制来计算每一种不同的组合。例如，如果我们有视觉（V）、文本（T）和音频（A）模态，那么我们会创建VT、VA、TA和AVT这些组合，以捕捉所有可能的跨模态交互。

Even after using an attention mechanism, a concatenation of the above cross-modal vectors is often performed to produce the fused vector F. Sum(.), max(.) even pooling operations may also be used instead.
即使在使用注意力机制之后，通常也会对上述跨模态向量进行拼接，以生成融合向量F。此外，也可以使用求和（.）、最大值（.）甚至池化操作来替代。

Classification 分类
Finally, once the fusion has been completed, vector F is fed into a classification model. This is usually a neural network with one or two hidden layers. The input vector F encodes complementary information from multiple modalities, thus providing a richer representation compared to the individual modalities V, A, and T. Hence, it should increase the predictive power of the classifier. 
最后，一旦融合完成，向量F就会被输入到分类模型中。这通常是一个带有一到两个隐藏层的神经网络。输入向量F对来自多种模态的互补信息进行编码，因此与单个模态V、A和T相比，它提供了更丰富的表示。因此，它应该能提高分类器的预测能力。

Mathematically, the aim of a unimodal model is to minimize the loss
从数学角度而言，单峰模型的目标是最小化损失。

‍ 
where 
 is an encoding function, typically a deep neural network, and C(.) is a classifier, typically one or more dense layers.
其中
是一个编码函数，通常是一个深度神经网络，而C(.)是一个分类器，通常是一个或多个密集层。
In contrast, the aim of multimodal learning is to minimize the loss 
相比之下，多模态学习的目标是最小化损失

where 
 denotes a fusion operation (e.g., concatenation) and 
 denotes encoding function of a single modality.
其中
表示融合操作（例如，拼接），
表示单一模态的编码函数。
