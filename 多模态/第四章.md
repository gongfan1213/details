https://slds-lmu.github.io/seminar_multimodal_dl/c03-00-further.html


# Chapter 4 Further Topics | Multimodal Deep Learning 网页总结
## 一、章节概述
本章聚焦多模态深度学习的进阶主题，突破此前文本与2D图像的局限，拓展至更多模态、结构化与非结构化数据融合、多用途模型及生成艺术四大核心方向，旨在探索更贴近人类感知能力的模型构建，同时探讨该领域当前成果、局限与未来研究路径。

## 二、核心章节内容
### 4.1 纳入更多模态（Including Further Modalities）
#### 1. 背景与必要性
- 文本和图像仅是人类感知连续多模态世界的“离散快照”，真实场景中还需处理视频（含时间维度）、音频、深度、温度等多类信号，且需让模型理解这些信号的底层结构。
- 避免罗列模态及对应SOTA模型的繁杂方式，提出基于“挑战”而非“模态类型”的分类体系，以更直观地理解多模态研究领域。

#### 2. 多模态挑战分类体系（Taxonomy of Multimodal Challenges）
| 挑战类型 | 核心内容 | 关键实现方式 |
|----------|----------|--------------|
| 多模态表示学习 | 学习能体现数据语义关联性的向量空间，核心是定义模态在训练/推理中的“协同性” | - **联合表示学习**：将多模态输入拼接为单一矩阵，模型学习统一表示（如视频的RGB图像与音频信号联合建模）<br>- **协调表示学习**：为各模态单独学习表示空间，通过损失函数对齐空间，使语义相似数据在不同空间中靠近（如文本查询与语音片段检索） |
| 多模态对齐 | 实现多模态同步（如音频与视频匹配），学习独立但可比较的表示空间 | 主流采用对比学习，使语义等效样本在嵌入空间中靠近、非等效样本远离 |
| 多模态融合 | 描述联合表示的学习过程，即模型内部合并模态信息 | 在Transformer架构中通过跨模态注意力实现，融合时机分为早期、中期、晚期或混合方式，需结合启发式、直觉、生物学合理性等设计 |
| 多模态转换 | 实现模态间映射，核心场景为检索与生成 | - **检索**：通过对比学习对齐单模态表示空间，实现跨模态查询（如文本查音频）<br>- **生成**：训练生成模型从向量空间解码出目标模态数据，需学习数据平滑分布，当前主流基于扩散模型（如DALL-E、GLIDE） |

#### 3. 当前研究趋势：广义自监督多模态感知
- **通用多模态架构**：需具备输入无关性、多模态融合能力、局部性保留、组合性尊重、灵活输出五大特性，代表架构如下：
  - **NÜWA**：将所有输入（文本、图像、视频等）转化为3D矩阵格式，通过3D近邻注意力机制（3DNA）保留几何与时间结构，支持灵活模态转换。
  - **Perceiver IO**：通过交叉注意力将大尺寸多模态输入压缩为小尺寸 latent 数组，支持处理长视频+音频等复杂输入，输出灵活性强。
  - **Hierarchical Perceiver**：在Perceiver IO基础上增加层级沙漏结构，增强局部性与组合性保留，降低计算成本。
- **多模态训练范式**：
  - **单模态无关自监督学习**：以BYOL和Data2vec为代表，通过 latent 预测（学生模型预测教师模型 latent 状态）实现，无需对比学习的难负样本，避免掩码自编码的解码需求，更适用于多模态场景。
  - **多模态自监督学习**：
    - 协调表示学习：如VATT通过MIL-NCE损失扩展InfoNCE，实现视频、音频、文本多模态的层级协调表示。
    - 联合表示学习：如MultiMAE对RGB图像、深度图等拼接输入进行掩码，通过多模态ViT骨干和任务解码器学习联合表示，同时支持单模态任务。
  - **个人研究**：提出通用非对比多模态表示学习框架，统一Perceiver输入格式与Data2vec的 latent 预测范式，可同时处理联合表示与协调表示任务，无需模态专属掩码策略或对比学习的难负样本。

### 4.2 结构化与非结构化数据融合（Structured + Unstructured Data）
#### 1. 背景与数据分类
- 此前多模态研究聚焦非结构化数据（图像、文本等），本章补充结构化数据（表格数据，如电子健康记录），二者在维度、可解释性上差异显著，需联合建模以避免丢失关键信息。
- 数据分类边界存在模糊性：如少量生物标志物为结构化数据，数千个生物标志物则因维度高、可解释性低归为非结构化数据。

#### 2. 融合策略（Fusion Strategies）
| 策略类型 | 操作方式 | 优缺点 |
|----------|----------|--------|
| 早期融合 | 输入层将多模态数据合并为统一特征向量，需预处理（如PCA降维）对齐维度 | 优点：早期融合信息，利用跨模态关联；缺点：预处理依赖专家知识或额外模型，易丢失细粒度信息 |
| 联合融合 | 模型不同深度融合模态，通过端到端学习从原始数据提取 latent 特征 | 优点：损失反向传播指导特征提取，灵活性高；缺点：需设计适配多模态的骨干网络（如CNN处理图像、LSTM处理序列） |
| 晚期融合 | 各模态单独训练模型，融合预测结果（如平均、多数投票） | 优点：模型独立性强，易部署；缺点：未利用模态间底层关联，性能通常低于前两种策略，本章暂不深入 |

#### 3. 应用场景
- **生存分析**：
  - 传统模型（如Cox比例风险模型）仅处理结构化数据，且假设协变量线性影响风险率，灵活性不足。
  - 多模态模型：DeepConvSurv用CNN处理病理图像预测患者风险；DeepCorrSurv加入分子数据，通过最大化模态子网输出相关性减少模态差异；MultiSurv整合6种模态，发现临床数据+mRNA数据组合性能最优，但多模态模型未必优于单模态结构化数据模型（如部分场景下不及传统CPH模型）。
  - 可解释性优化：Wide & Deep NN结合线性部分（记忆特征）与FCNN部分（泛化特征），DeepPAMM通过正交化解决特征重叠导致的可识别性问题，保留模型可解释性。
- **经济与金融领域**：
  - SSDDR模型：扩展结构化加性分布回归（SADR），支持用线性、GAM或NN模型学习分布参数，通过正交化单元解决Wide & Deep NN的特征重叠问题，同时实现不确定性量化。
  - 替代数据应用：用卫星图像（非结构化）预测非洲贫困率、美国社会经济属性，替代成本高、获取难的结构化调查数据；用CNN检测非洲建筑、Deep Gaussian Processes预测美国玉米产量，可与结构化数据结合提升预测能力。

#### 4. 结论与展望
- 现有成果：端到端学习优于独立特征提取，多模态模型通常优于单模态模型，但结构化数据仍是多数任务的核心信息源，非结构化数据仅起补充作用。
- 局限：部分研究未与单模态模型对比，多模态模型超参数多、模态选择灵活易导致结果偶然性，小样本场景（如罕见病医学图像）性能不稳定，存在发表偏倚。
- 未来方向：需积累大规模高质量多模态数据，聚焦关键模态（而非盲目增加模态数量），提升模型鲁棒性。

### 4.3 多用途模型（Multipurpose Models）
#### 1. 基础概念
- **多任务学习**：模型同时训练多个任务（如目标识别、机器翻译），利用任务间相似模式提升泛化能力，但部分任务可能存在资源竞争导致性能下降。
- **混合专家模型（MoE）**：通过稀疏激活专家子网（而非全网络激活）提升模型规模同时控制推理成本，需设计门控网络选择适配专家，并平衡专家负载。
- **进化算法**：通过种群变异、重组、筛选优化离散空间问题，需平衡探索（新解空间）与利用（已知优解）。
- **多用途模型定义**：融合多模态与多任务学习的模型，模拟人类“多感知+知识迁移”能力。

#### 2. 主流多用途模型对比
| 模型名称 | 核心方法 | 支持模态 | 性能表现 | 关键特点 |
|----------|----------|----------|----------|----------|
| MultiModel（2017） | 模态网络（转换数据格式）+ 编码器+I/O混合器+解码器，MoE层增强 | 视觉、文本、音频、分类数据 | 多任务性能低于SOTA单任务模型，但低资源任务优于单任务模型 | 预Transformer时代架构，输出仅支持文本与分类，易扩展模态 |
| UniT（2021，201M参数） | 多模态编码器（BERT处理文本、DETR处理图像）+ 任务查询解码器+任务头 | 视觉、文本 | 多模态任务优于单任务版本，单模态任务弱于单任务模型，性能接近SOTA | 基于Transformer，通过任务令牌和任务头聚焦任务，易扩展编码器 |
| OFA（2022，33M-930M参数） | 序列化所有输入（图像分块、文本BPE编码），Transformer seq2seq架构 | 视觉、文本 | 跨模态任务（图像 caption、视觉问答）优于SOTA，单模态任务接近SOTA，支持零样本迁移 | 统一seq2seq框架，无需模态专属模块，需额外模块解码令牌（如生成图像） |
| Gato（2022，79M-1.18B参数） | 序列化所有输入（图像ResNet编码、文本SentencePiece编码），Transformer解码器 | 视觉、文本、机器人控制、离散实体（如游戏按钮） | 多数未见过的任务优于从头训练的单任务模型，Atari Boxing任务弱于单任务模型，机器人控制接近SOTA |  autoregressive预测动作，需分隔令牌区分观测与动作，输入长度限制零样本能力 |

#### 3. 路径与前沿工作
- **Pathways提案**：构建大型节点网络（节点为子网），推理时仅激活任务适配的子网路径，提升效率与泛化能力，配套Google数据中心框架优化稀疏计算，衍生PaLM（540B参数语言模型）、Minerva、Parti等模型。
- **PathNet**：用进化算法寻找任务最优路径，冻结已学路径参数避免灾难性遗忘，提升多任务训练效率与预测质量。
- **LIMoE**：单塔Transformer架构，MoE层替代传统FFN层，通过门控网络选择专家，结合局部/全局熵损失平衡专家负载，在ImageNet、COCO数据集上优于CLIP，参数效率更高。
- **muNet**：用进化算法变异预训练模型，通过层克隆、插入、删除、超参调整生成任务适配模型，保留原知识，参数少于传统微调模型且性能更优。

#### 4. 讨论与挑战
- 可访问性低：大模型需API调用，普通GPU难以运行，需开源协作（如Hugging Face）与模型蒸馏技术。
- 缺乏统一指标：现有指标不适配多任务多模态模型，跨模态任务（如机器人控制）评估难度大。
- 社会影响：多数据集引入偏见，大模型训练碳排放高，需关注伦理与环境问题。

### 4.4 生成艺术（Generative Art）
#### 1. 发展历程
| 时间 | 关键技术/模型 | 特点 |
|------|---------------|------|
| 2015 | DeepDream | 首个AI生成图像工具，基于CNN层激活生成抽象图像 |
| 2016 | 神经风格迁移（Gatys等） | 实现图像风格迁移（如将凡高风格迁移到LMU校徽） |
| 2014（提出）-2019（优化） | GAN/StyleGAN | StyleGAN生成高真实感假图像（如不存在的人脸），但无法精确控制输出 |
| 2021 | CLIP+BigGAN/StyleGAN/VQGAN、DALL-E、GLIDE | 多模态模型结合“艺术家-评论家”范式（生成模型为艺术家，CLIP为评论家），支持文本Prompt控制输出，GLIDE通过扩散模型提升生成精细度 |

#### 2. 模型使用方式
- 无需编程基础：通过公开Notebook（输入文本Prompt运行）、API或GUI（如MindsEye beta）即可生成图像。
- Prompt工程技巧：微调文本指令影响输出（如加“unreal engine”提升图像真实度），因模型训练数据含该引擎相关图像。
- 开源限制：DALL-E未开源，仅存在俄语开源版ruDALL-E、Hugging Face托管的DALL-E mini，CLIP训练数据未公开，LAION-400m为开源CLIP嵌入数据集。

#### 3. 任务与模态扩展
- 核心任务：文本生成图像（从噪声或已有图像初始化，如CLIP+VQGAN基于LMU校徽生成凡高风格图像）、图像编辑（如GLIDE文本条件修复、基于草图填充细节）、图像检索与裁剪。
- 模态扩展：可结合音频（如WZRD为视频配音频）、3D打印（生成雕塑）等模态。

#### 4. 讨论与展望
- 创作角色争议：AI作为创作辅助工具（生成多版图像供人类筛选），但随能力提升人类干预减少，暂无法定义“美学损失函数”量化图像美感。
- 风险：高真实度生成内容可能用于抄袭、传播虚假信息，部分模型（如DALL-E）因风险未完全开源。
- 偏见与伦理：模型受训练数据偏见影响（如CLIP将“人类”更多关联男性），需优化数据与模型；生成艺术可制作NFT（加密艺术），但市场新兴且争议大。
- 未来方向：技术持续升级（如DALL-E 2性能优于DALL-E 1），将进一步融合多模态与提升生成质量。

## 三、章节总结
本章从多模态扩展、数据融合、多任务模型、生成艺术四大维度，系统梳理多模态深度学习的进阶方向：通过“挑战分类”重构多模态研究框架，提出结构化与非结构化数据的融合策略，对比多用途模型的架构差异，追溯生成艺术的技术演进。同时，指出当前领域在小样本鲁棒性、模型可解释性、伦理风险等方面的局限，为未来研究提供“通用化模型”“关键模态聚焦”“偏见缓解”等核心路径。
