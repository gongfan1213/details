### 面试题：LLM 适配与 Function-Call

#### Q1：如何在不同模型之间复用工具调用协议？
**标准回答**：
- 在 `LLM` 层进行“工具序列化抽象”，将本地工具与 MCP 工具统一为 `tools` 数组；对不同厂商差异（如 Claude 对 tools 的特殊格式）在适配层做转换，不影响上游编排器与下游工具服务。

#### Q2：如何控制 Token 预算与防止越界？
**标准回答**：
- 配置 `max_tokens` 与 `max_input_tokens`；在调用前对消息（含系统提示与历史对话）进行 `TokenCounter` 粗略统计与截断；对长文档采用“摘要/片段化”与文件抽取（如 CSV/HTML 抽取前若干行）降低输入成本。

#### Q3：如何把工具参数的 JSON-Schema 与 Java 类型映射起来？
**标准回答**：
- `BaseTool.toParams()` 输出 JSON-Schema；MCP 工具的 `inputSchema` 直接透传；在执行前做参数校验与类型转换（如 `number/string/enum`），对可选字段提供默认值与边界检查；对不确定结构的工具在 Prompt 中加强自然语言参数约束。

#### Q4：如何提升 Function-Call 的可解释性与稳定性？
**标准回答**：
- Prompt 工程：分离“思考/计划/调用/总结”输出，控制输出格式；在错误重试时引导模型给出原因与纠正。
- 执行工程：记录每次工具输入与输出片段，便于审计与回放；限制同类工具在短时间内的连环调用；必要时锁步（先读取→再生成→最后汇总），防止竞态。

#### Q5：当模型返回无效工具或参数不全怎么办？
**标准回答**：
- 参数校验失败时回写“参数不完整/不合法”并要求模型重试；对于不存在的工具名，返回“未知工具”并给出可选列表；对于结构不匹配的复杂参数，建议模型先生成 JSON 示例，再进行校验与修正后重试。


