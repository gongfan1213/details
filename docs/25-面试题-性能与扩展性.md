### 面试题：性能与扩展性

#### Q1：SSE 大量长连接下如何扩展？
**标准回答**：
- 前置反向代理（Nginx/Envoy）做连接复用与限流，后端维持较少的上游连接；关闭代理缓存，调大 `proxy_read_timeout`；启用 HTTP/1.1 keepalive。
- 后端拆分“编排服务（SSE）”与“工具回调服务”，避免重活堆积在同一进程；对 `/web/api/v1/gpt/queryAgentStreamIncr` 使用轻量线程池（或 Reactor/Servlet3.1 非阻塞）。
- 心跳间隔折中（10-15s），客户端断线重连指数退避；限制单用户最大并发连接与单 IP 总连接数。

#### Q2：如何处理流式回传的背压与节流？
**标准回答**：
- 统一在 Python 工具端支持 `stream_mode`：`general|token|time`，按批/按时间片推送；后端中转时合并小片段，减少 write 次数。
- 前端使用 `requestAnimationFrame` 合并渲染，避免每 token 触发重排；对超长文本分段虚拟化。

#### Q3：多智能体执行的并发与隔离如何设计？
**标准回答**：
- 以 `requestId/sessionId` 为粒度隔离内存与临时文件目录；工具执行独立进程/线程池，避免阻塞编排主线程。
- 对“计划→执行→总结”串行主线中的工具调用做并行上限（如同一任务内最多 N 个工具并行）；对潜在幂等操作做去重（同一参数不重复调用）。

#### Q4：LLM 调用的超时、重试与降级策略？
**标准回答**：
- 配置 OkHttp/HTTP 客户端的 `connect/read/write` 超时；对 429/5xx 进行指数退避重试；达阈值后降级为非流式/简化提示词/替代模型（`llm.settings` 切换）。
- 对大输入做截断与摘要（TokenCounter + 文件摘要）；对失败分支记录参数与响应片段，便于回放与剖析。

#### Q5：静态与动态扩容方案？
**标准回答**：
- 水平扩容：后端与工具服务多副本 + 共享会话存储（依赖 `requestId` 去状态化）；前端静态资源走 CDN。
- 垂直扩容：提高 Python worker 数量（`uvicorn --workers`），调整 JVM 堆与线程池；对大算力模型调用放置到独立任务队列或派生服务。

#### Q6：文件服务的吞吐与大文件处理？
**标准回答**：
- 大文件采用直传（多段上传）与异步处理；下载/预览通过 `FileResponse` 流式回传并设置合适的 `Content-Type`，避免一次性读入内存。
- 结果文件命名唯一化（`requestId+filename`）并放置到分层目录；清理策略按 TTL 扫描删除临时产物。

#### Q7：容量规划与指标基线如何设定？
**标准回答**：
- 基线指标：并发连接数、SSE 平均存活时间、每秒事件数、LLM QPS/延迟/错误率、工具队列长度、内存与 fd 使用率。
- 结合峰值流量与模型延迟做“事件生产速率 vs 消费速率”匹配；压测覆盖：千连接、慢模型、超大输出、集中上传。


