### 12 Prompt 评测与 Guardrails 面试题（Prompt Evaluation & Guardrails）

- 关注点：提示词质量、结构化输出、越权与注入防护、自动化评测与回归
- 关键参考：`prompts/**`, `THOUGHT_CONTROL_GUIDE.md`, `utils/langfuse_llm_client.py`, `config/llm_config.yaml`

#### 基础题
- 本项目如何组织 Prompt 模板？系统提示、少样本示例、占位符变量如何分层管理？
- 结构化输出如何保证？（JSON Schema、正则/语法约束、再解析与重试）
- 思维控制：参考 `THOUGHT_CONTROL_GUIDE.md`，你如何控制“思考-行动-总结”的边界与合规？

#### 进阶题
- 对抗与注入：如何识别 Prompt 注入与工具滥用？在工具调用前后如何进行“上下文白名单 + 来源标记 + 最小可用信息”保护？
- 自动化评测：如何基于 `Langfuse/Langsmith` 建立 Prompt 变更回归评测集（覆盖率、稳定性、退化检测）？
- 成本与延迟：通过裁剪/摘要/压缩/检索增强，如何在不降质的情况下减少 token？

#### 实操题
- 设计一个“评测用例集”结构（输入、期望结构、验收脚本），可在 CI 中自动跑并出分。
- 为 `requirement_alignment_agent` 写一组“注入攻击样例”，并说明防护命中后应如何回复（拒绝 + 合规引导）。

#### 场景题
- 生产事故：Prompt 被误改导致大规模偏离，如何自动回滚与禁用变更？如何在 Apdex/SLO 降级时触发直达告警？
- 多语言：如何设计 Prompt 使中英文皆稳（占位符、术语词典、格式指令）？

#### 追问
- 如何定义“可解释性”指标并在评测报告中呈现（引用率、理由完整度、反事实分析）？
- 强约束输出失败时的兜底策略（自动重试、放宽约束、触发人审）。

#### 作业
- 写一个“结构化输出校验与重试”伪代码：尝试 N 次 JSON 解析失败则降级输出，并生成可观测事件。

#### 图稿
- 参见：`interview/diagrams/prompt_guardrails.md`。

#### 样例回答/评分标准
- 样例回答要点：
  - Prompt 分层（系统提示/少样本/变量）与版本治理；
  - JSON Schema 校验 + 重试策略；
  - 注入防护（来源标记/白名单/最小上下文）与违规处置；
  - 评测集构建、覆盖率、稳定性阈值与回滚条件。
- 评分标准：
  - 优秀：能给出评测集与自动门禁方案，描述防护与回滚闭环；
  - 合格：能系统化阐述校验/重试/评测，给出关键指标；
  - 待提高：缺少结构化输出与安全边界设计。

### 常见错误与改进建议
- 错误：提示词变更直接上线，无评测与回滚。
  - 改进：小样本评测集 + CI 门禁，失败自动回滚与禁用。
- 错误：结构化输出仅靠正则匹配，脆弱。
  - 改进：JSON Schema 校验 + 重试 + 降级策略；失败事件可观测。
- 错误：未区分“系统指令/用户内容/外部引用”，易受注入影响。
  - 改进：分层拼装与来源标记，仅将外部内容作为引证。

### 参考答案（示例）
- 提示词组织：
  - 系统提示（边界/责任/风格） + Few-Shot（高质量示例） + 变量占位符（领域/长度/平台等）；
  - 版本管理与回滚：Prompt 变更需评测通过后上线；
- 结构化输出：
  - 生成 JSON + Schema 校验，不通过自动重试 N 次；
  - 解析失败记录事件（trace/span + metrics），降级为宽松模式或触发人审；
- 注入防护：
  - 外部文本仅做引证并标记来源；对危险指令严格拒绝并返回安全引导；
- 评测与门禁：
  - 构建小样本评测集（代表性用例），CI 中跑并设置阈值；
  - 线上监控转化/一致性/引用率，退化自动回滚。
