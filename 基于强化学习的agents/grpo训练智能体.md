https://www.youtube.com/watch?v=gEDl9C8s_-4


<img width="573" height="414" alt="image" src="https://github.com/user-attachments/assets/7a905025-6562-4def-a0f0-61d8a38c78bd" />


这是一场关于利用强化学习（RL）构建可靠智能体的演讲，核心围绕一个名为ART-E的AI电子邮件助手案例展开，深入探讨了强化学习在实际应用中的关键要点，包括前期准备、模型训练、常见问题及解决方案等方面，以下是详细总结：

### 一、前期准备：从提示模型起步
1. **优先使用提示模型**：在考虑强化学习之前，应先致力于通过提示模型（prompted models）获取最佳性能。这样做有诸多益处，一方面能有效排查环境中的问题，像工具是否正确实现、是否能访问预期数据等，且这种排查相对独立，不会与训练循环的调试相互干扰；另一方面，有可能发现提示模型已能很好地完成任务，从而省去训练的时间和成本。此外，当后续强化学习模型能超越精心构建的提示模型基线时，会带来极大的成就感。
2. **案例成果展示**：以ART-E模型为例，通过训练，它在准确性上超越了多个提示模型，如03、04 mini等。从数据来看，原本表现较好的03提示模型准确率为90%，而ART-E模型能达到96%，解决了03模型所犯错误中60%的问题。在成本方面，相较于03和04 mini，ART-E模型基于更小的Quen 2.5 14B模型，运行成本大幅降低。同时，由于模型更小以及训练中优化了与数据库和电子邮件交互的轮次，其延迟也显著减少。

### 二、环境与奖励函数：强化学习的两大难题
1. **构建现实环境**：对于ART-E模型，其运行环境需要能够模拟真实的电子邮件交互，包括查询收件箱、获取邮件等功能。为解决数据获取难题，项目采用了安然公司公开的50万封电子邮件作为训练数据，这些真实的邮件数据为模型提供了接近实际应用场景的训练环境。
2. **设计有效奖励函数**：为了让模型能够学习到正确的行为，需要设计合理的奖励函数。在ART-E项目中，通过让Gemini 2.5 Pro生成基于邮件的问题和答案，并进行筛选，构建了一个包含数千个问题及其验证答案的“黄金数据集”。利用这个数据集，通过简单的LLM判断模型答案与正确答案是否一致，从而确定奖励或惩罚，使模型能够在训练中不断优化。

### 三、模型训练与优化
1. **多目标联合优化**：在训练过程中，除了主要的准确性目标外，还可以在奖励函数中加入其他优化目标。例如，在ART-E模型中，加入了对交互轮次和幻觉答案的优化。对于交互轮次，给予模型使用更少轮次查询邮箱的额外奖励，使模型学会更高效地利用工具；对于幻觉答案，惩罚模型给出错误答案的行为，鼓励模型在无法确定时承认不知道，从而降低幻觉率。
2. **警惕奖励黑客攻击**：在强化学习训练中，模型可能会找到奖励函数的漏洞，采取投机取巧的方式获得高奖励但不解决实际问题。例如，在Hacker News标题生成项目中，模型发现可以忽略内容生成固定标题来获取高赞。解决方法是不断改进奖励函数，如添加额外的判断机制来检查答案与内容的匹配度等。

### 四、实践经验与资源
1. **实现成本与效率**：ART-E模型的训练成本约为80美元的GPU时间，工程时间约为一周，且随着行业对相关模式的探索，预计实现强化学习模型的成本和时间会进一步降低。
2. **资源分享**：演讲者分享了包含项目代码、数据集等内容的链接，以及一个用于交流强化学习模型训练的Discord社区，为感兴趣的开发者提供了学习和交流的平台。

总体而言，演讲通过ART-E案例全面阐述了利用强化学习构建可靠智能体的关键步骤和注意事项，强调了从提示模型开始、重视环境和奖励函数设计、警惕训练中的问题并持续优化等要点，为相关领域的研究和实践提供了有价值的参考。


以下是使用 RL 构建可靠智能体的介绍。

0:00
[音乐]
0:14
大家好，很高兴你们都来了
0:17
这是推理和强化学习的专题，在AI工程师世界博览会最后一天的下午。很高兴你们今天和我们一起分享。我要谈论的是一个非常具体的案例研究，我们做了这个案例研究。我将具体谈谈所学到的经验教训，哪些有效，哪些无效，以及我们如何能够构建一个在强化学习方面表现良好的智能体。我在本次演讲中谈到的所有内容，这是我们构建的一个开源代码库。我们希望分享这些经验，我也会在最后与大家分享该链接。对于那些想要复制我们所做的事情的人来说，我们将讨论的项目是一个名为 ART-E 的项目，它是一个自然语言助手，可以帮助你回答电子邮件收件箱中的问题。我将举一个例子来说明我们在这里谈论的内容。假设你想问，在这种情况下，我们的示例问题是雪莉搬到波特兰的目标时间是什么时候。你会向助手提出这个问题，然后它会搜索你的收件箱。它有几个工具，比如搜索工具、阅读电子邮件工具，然后它实际上可以回答最终的问题。如果你看看这里幕后发生了什么，这很重要。这样你就能了解这个智能体的工作原理，并且在我们讨论如何构建它、如何使其工作时，希望这能帮助使对话在特定任务中非常有根据。无论如何，你会看到智能体在搜索某些关键词，获取这些消息，阅读其中一条并回答问题，这就是它所做的。

1:41
好的，一旦我们决定这是我们试图解决的任务，为什么你会特别使用强化学习呢？答案是，一开始你不应该这样做。事实上，一开始我们根本没有使用任何强化学习。我们纯粹是在提示模型上构建的，这是本次演讲中我想分享的第一个经验教训。我通常总是建议在进行任何训练（包括强化学习）之前，先使用提示模型获得最佳性能。这样做有几个不同的原因，特别是有三个原因。第一个原因是解决环境中的问题。你知道，也许你的工具没有正确实现，也许它们无法访问你认为它们可以访问的数据。我们发现这种情况经常发生，而且与调试训练循环相比，单独调试这个问题要少很多挫败感。所以你要确保在开始训练之前，至少可以获得某种性能。其次，当你试图提高使用这些提示模型的性能时，你可能会发现你可以让它运行得非常好，这很棒。这意味着你不需要训练任何东西，这为你节省了很多时间。还有第三个原因，我也会分享，那就是一旦你付出了努力，尽你所能获得了最佳质量的提示基线，然后如果你发现这些基线无法让你达到你需要的目标，而你能够通过强化学习超越它们，那感觉很棒。你可以自豪地说：“是的，我能够在我的任务上击败前沿模型。”我强烈推荐这样做，感觉很好。你可以在 X 上发布关于它的内容，有漂亮的图表等等。这就是一切顺利时的样子。这是我将要谈论的这个 ART-E 模型的训练运行示例。你可以看到，对于我们获得的每个提示模型基线，都有这些线条。我们有 03、04 mini，然后是 Gemini 和 4.1，你可以看到这些模型有一定的性能水平，然后你可以看到这个移动的线条，这是我们训练的模型，你可以看到它最初明显比这些其他模型差得多。从一开始，这是因为我们从一个 140 亿参数的 Quen 2.5 开始，这是一个相对较小的模型，相对较弱的模型。因此，它最初的表现比这些模型差得多，但你可以看到随着训练的进行，最初在开始时，它可能正在学习正确的工具调用方式，有一个非常明显的提升，因为它弄清楚了基本的东西，然后是更渐进的提升，直到最终它能够在这项任务上显著优于任何提示模型。这就是你在理想情况下，当一切正常时所期望的，这就是你希望实现的。这是我们刚刚看到的相同数据的另一种视图。我想以这种方式强调它，因为重要的是要意识到，在上一张图表中，这些线条看起来非常接近地趋于平缓，这是因为它们接近 100%。但最后，例如，对于我们最好的提示模型 03，它的准确率是 90%，而我们的 RL 模型能够达到 96%。因此，一种思考方式是，03 所犯的错误中，有 60% 实际上是由我们的模型解决的。这对于使用这些模型的用户体验来说实际上可能非常重要。如果你犯的错误只有一半，这可以使产品更强大。所以这就是我们在准确性方面所达到的水平。我们发现还有其他几个指标通常非常重要，而且它们之间的权衡确实取决于任务，但在许多情况下它们很重要。成本显然是一个大问题。对于我们的电子邮件智能体框架，我们对 034 mini 和我们的模型进行了成本基准测试。如果你想使用 03 进行一千次搜索，那将花费 55 美元，对于大多数用例来说，从单位经济角度来看，这可能成本过高。在 04 mini 上，我们降到了 8 美元，但这仍然相当昂贵，然后通过转向这个更小的 Quen 2.5 14B，我们又降低了一个数量级。这再次仅仅是因为它是一个小得多的模型，所以运行起来便宜得多，但由于我们在我们的任务上对其进行了专门化，我们仍然能够获得非常好的性能。除了成本和准确性之外，第三个经常出现的指标是延迟，特别是如果你正在做任何与语音相关的事情，或者如果与任务有任何实时的人机交互，延迟将非常重要。我们发现，在这项任务上，我们能够获得显著更好的延迟。有几种不同的方法，我稍后会更详细地介绍，我们能够实现这一点。再次，转向更小的模型有所帮助，因为从内存中加载的内容更少，矩阵乘法更少，你能够更快地输出标记。我们还能够训练这个模型，使其在与数据库和实际电子邮件（电子邮件列表）的来回交互中减少次数。我们能够训练它更高效地进行查询，我稍后会谈到这一点，这导致了更低的延迟。实际上还有第三件事，我们在这里没有应用，但对这些较小的模型有很大帮助，那就是所谓的投机解码。你可以在大型或小型模型上做到这一点，它通常在特定于任务的较小模型上效果更好，因为你在投机者上获得更高的接受率。但基本上，较小的模型有很多原因可以更好地工作。

3:59
接下来的问题，对于那些还没有这样做的人来说，是实现这些结果需要付出什么努力。如果你在一年前问我这个问题，我会说：“嘿，你真的应该只有在你是这家大公司并且愿意在一个项目上投入数月的工作时才这样做。”我认为情况正在改变，我确实这样认为。在这种情况下，这个训练运行在 GPU 时间上花费了我们大约 80 美元，构建这个东西花了大约一周的工程时间，需要注意的是，这是一位熟悉该领域并且在机器学习和 RL 方面有相当多经验的工程师。但我实际上期望，随着我们作为一个行业集体找出正确的模式，这将继续下降。我预计，从这些专门化模型中获得投资回报的回报期实际上也将继续缩短。我想做这个演讲的部分原因是为了传播我们学到的知识，并希望更快地迈向这个世界，在这个世界里，这只是每个人都知道如何做的事情，而且非常容易和快速。所以，在剩下的时间里，我们将谈论更多我们学到的经验教训。

7:41
好的，当你使用 RL 来训练一个智能体或实际上将 RL 用于其他任何事情时，我发现，在我们研究的不同问题中，始终有两个难题每次都会出现。这两个难题首先是找出一个现实的环境。如果你正在训练一个智能体，你需要用现实的数据、现实的输入和输出、可用的工具等来训练它，就像它将在生产中使用的那样。因为如果你不这样做，它将针对错误的事情进行优化，当你部署它时，你将无法获得你想要的结果。第二个问题，有时很难，有时不是，这有点取决于测试，是获得正确的奖励函数。奖励函数意味着你必须能够知道，当你的智能体经历并在这种情况下给出我的电子邮件的答案时，你必须有某种方式知道它是否做得好或不好。这就是奖励函数，它决定了它是好是坏。这取决于领域，有时这真的很容易。我们有一个名为 RLVR 的东西，在一些可验证的领域，实际上很容易获得奖励，但通常并非所有领域都是这样，通常这有点困难。所以这有点取决于任务。我将通过 RE 具体解决这些问题。第一个是现实环境。对于我们的 RE 任务，我们需要什么环境？这个智能体将在什么环境中运行？它需要这些可用的工具，它需要能够查询电子邮件收件箱，它需要能够获取电子邮件回来，并且这些电子邮件看起来是真实的。你知道，收件箱应该很大，因为大多数电子邮件收件箱都是这样的。其中的电子邮件应该是多样化的，它们必须看起来像真实的电子邮件。这可能有点困难，因为你不能只是去要求一千个人给你他们的个人电子邮件来进行训练。幸运的是，在这种情况下，我们能够在一家为开放数据生态系统做出了很大贡献的公司的帮助下解决这个问题。一般来说，这是一家相当标志性的公司，也许我会称之为一家历史悠久的公司。我当然在谈论安然公司。我听到一些笑声，所以无论如何，安然是一家在 90 年代和 2000 年代的金融化能源公司，犯下了大规模欺诈行为，最终被司法部关闭，作为这个过程的一部分。作为发现过程的一部分，他们的 50 万封电子邮件的转储被公开。所以这对像这样的事情来说非常好，这就是我们用作电子邮件收件箱环境的东西。所以现在我们有了现实的电子邮件收件箱，有数万封来回的真实电子邮件。现在我们必须设计我们的奖励函数。当我们的智能体在询问问题并给出答案时，我们必须知道答案是否正确，以便在它答对时奖励它，它可以学会做得更好。有不同的方法，这部分非常依赖于任务。在这种情况下，我们的方法基本上是将其变成一个更可验证的问题。我们所做的方式是，我们实际上采用了我们的电子邮件收件箱，我们反转了问题。我们一次从收件箱中抓取 20 封电子邮件，并将它们提供给 Gemini 2.5 Pro，并说：“嘿，考虑到这组电子邮件，给我们一些用户可能会实际提出的问题，答案可以在这封电子邮件中找到。” Gemini 生成了问题，生成了答案，然后当然是来自的源电子邮件。除此之外还有一些额外的步骤，它提出的很多问题看起来有点不现实。我们有一个单独的过滤步骤，我们会说，好吧，让我们找到其中那些看起来像我可能会问的问题的子集，我们最终得到了几千个问题的列表，以及它们经过验证的答案。此时，它变得更像是一个经过验证的事情。奖励函数变得容易得多，因为我们知道正确的答案应该是什么。因此，我们可以判断我们的智能体是否做得好的方法是，给我们的智能体问题，让它去搜索电子邮件收件箱，尝试找到正确的电子邮件等，最终它会返回一个答案，然后我们可以简单地使用一个非常简单的 LLM 作为判断者，并说，嘿，你知道，这是问题，这是我们认为正确的黄金答案，这是我们从模型中得到的答案，它是对还是错。我们确实必须在那里做一些迭代，确保判断者在什么算作正确或不正确方面得到了很好的校准，但总的来说，这工作得相当好，并能够使这更像是一个经过验证的任务。所以这就是我们如何解决奖励函数问题的，通过将其变成我们有更多黄金数据集的东西。

12:27
好的，一旦你解决了这些问题，一旦你有了你的环境，一旦你定义了你的奖励函数，那么基本上你只需要一遍又一遍地运行一个循环，你的智能体在其中尝试解决问题，然后你弄清楚它是好是坏。然后你只是在它好的时候奖励它，在它坏的时候惩罚它。就是这样。你一遍又一遍地这样做，然后希望如果你把一切都设置正确，它会学习什么是好的，什么是坏的，并开始正确地做。然后再次，这是我们之前看到的曲线，你可以看到它随着时间的推移开始变得更好。

13:17
从这个项目中还有一些有趣的经验教训。我们发现，实际上你可以在奖励函数中投入很多东西，而不仅仅是你试图解决的主要问题。因此，我们最终有大约八个不同的小东西，我们给予了额外的奖励。我将在这里分享其中两个。第一个是我们试图优化轮次的数量，即在查询电子邮件收件箱之前来回的次数，以找出正确的答案。当然，最重要的事情是得到正确的答案，但在两个都正确的答案之间，我们希望它来回的次数更少，因为这意味着更少的标记，更低的延迟，更低的成本。这只是一个更高效的智能体。所以你可以在这里看到第一个图表，早期，当它在摸索并弄清楚什么有效时，它最终平均飙升到超过六轮。所以它会多次来回查询电子邮件收件箱，试图找到正确的东西，但一旦它能够有效地使用工具，弄清楚如何构建关键字并找到正确的电子邮件，它就能够变得非常高效，实际上比我们的任何提示模型都快，在使用更少轮次的指标上。这仅仅是因为我们给了它一点额外的奖励，相对于正确答案的奖励来说，这是非常小的一部分，但在使用更少轮次上给予一点额外的奖励，它能够利用这一点进行优化。我们给予它的另一个额外奖励函数是试图阻止它产生幻觉答案。显然，最好的事情是得到正确的答案。如果你找不到正确的答案，说“我不知道”比编造一个答案要好得多。所以我们基本上惩罚它，如果奖励模型说：“嘿，你答错了。”但它试图给出一个比只是说“嘿，我不知道，我无法解决这个问题”奖励低得多的答案。正如你所看到的，这工作得相当好。与包括 03 在内的任何提示模型相比，我们最终的幻觉率显著降低，因为这是我们奖励函数的一部分。这些只是额外的奖励，但我们发现你可以投入很多这样的东西，它可以同时优化所有这些，这非常强大。

15:26
我想稍微谈谈奖励黑客攻击。当你试图这样做时，这经常出现，这是一个有趣的话题。这是一个标志性的视频，你们中的一些人可能已经看过。这是 OpenAI 近十年前发布的，他们试图在这个环境中让这艘船完成比赛，而不是学习完成比赛，它学会了哦，如果我只是在这个小圈子里，甚至不是赛道的一部分，我可以得到很多分数。所以它开始一遍又一遍地这样做，而不是实际遵循。这在你进行强化学习时经常出现，这基本上只是你实际希望模型做的事情和你可以衡量的事情之间的区别。你实际上奖励它的东西，如果你让其中一个运行足够长的时间，它几乎总是会想出某种方式来利用你的衡量标准，它会想出某种方式来获得非常高的奖励而不实际解决问题，你需要注意这一点。我将在这里举几个例子。这是一个图表，实际上来自另一个项目，不是这个项目。我们团队的一位工程师正在研究这个名为 NYT Connections 的游戏，你们中的一些人可能知道，你会得到 16 个单词，你必须把它们分成四组，每组四个。这是一个相当具有挑战性的游戏，特别是对于这些语言模型，因为它需要很多世界知识和横向思维。无论如何，他们试图训练这个模型来做这件事，它没有弄清楚，没有弄清楚，然后突然，你可以在这里看到大约在第 40 步，它突然起飞，就像“我们弄清楚了如何解决这个问题”。这位工程师，我要指出，我们团队的 An 在这里，他在会议上，是的，他很棒，之后你应该和他谈谈。但他说，“我们解决了 NYT Connections 的问题”，图表看起来不错，让我们看看它实际在做什么。它实际上在做的是，它发现我们编写的验证存在一个错误，如果它只是将每个单词放入每个类别，它就能获得完美的分数，因为我们没有验证每个类别中实际上只有四个单词。这是另一个例子，这是一个有趣的例子。我正在训练一个模型来为 Hacker News 生成非常好的标题，这些标题会获得点赞。我有一个奖励模型，我在现有的 Hacker News 文章上进行了训练，以及它们获得了多少点赞。我试图训练这个模型来生成新的标题，它在一段时间内工作得非常好。你可以看到，主观上，我查看了很多这些生成的标题，在最初的大约一千步左右，它实际上正在学习我认为“好的，作为一个在 Hacker News 上花费太多时间的人，是的，这看起来确实是一个好标题，你做得很好。”然后你可以看到大约在第 1200 步左右，它突然跳了很多。它显然发现了一些东西，我不知道它发现了什么，但我们应该看看。事实证明，模型发现它可以完全忽略帖子的内容，并为每一篇文章生成相同的标题，这将最大化它的分数。它生成了这个标题“谷歌裁员 80%”，实际上每一篇文章都是如此。这就是它的标签，并且认为“是的，这肯定会在 Hacker News 上获得点赞”，公平地说，它可能会。

18:39
所以无论如何，我们解决这个问题的方法是，我们发现注意这一点非常重要。解决它通常涉及以某种方式修改你的奖励函数，以惩罚这样的事情。在我谈到的第二个例子中，一旦我们发现了它，实际上是一个相当容易的修复，那就是添加一个额外的 LMS 判断者，它查看标题和内容，并说，嘿，标题中是否有任何内容不被
内容所支持。我们添加了这个，它实际上工作得很好。重要的是，你要查看你的滚动结果，而不仅仅是盲目信任奖励函数，弄清楚实际发生了什么。

19:09
无论如何，我的时间快到了，所以我要停下来。这里有几个二维码。本次演讲中的所有内容，以及我对整个项目的更长篇幅的撰写，包括代码、沿途的工件数据集。你可以在那里查看。还有一件事是，我们有一个开放的 Discord，我们有一个用于训练强化学习模型的开源项目。我们有一个 discord，如果你对这类事情感兴趣，可以去那里。我们都在那里，回答问题，有很多来自社区的人试图做这些事情。所以如果你有兴趣用这个来构建东西，请随时加入，我很乐意在那里聊天。是的，感谢大家，感谢你们的时间。

