https://zerozzz.win/an-agentic-approach-to-reducing-llm-hallucinations-by-youness-mansar-dec-2024-towards-data-science


If you’ve worked with LLMs, you know they can sometimes hallucinate. This means they generate text that’s either nonsensical or contradicts the input data. It’s a common issue that can hurts the reliability of LLM-powered applications.

如果你使用过大型语言模型，就会知道它们有时会产生幻觉。这意味着它们生成的文本要么毫无意义，要么与输入数据相矛盾。这是一个常见问题，可能会损害由大型语言模型驱动的应用程序的可靠性。

In this post, we’ll explore a few simple techniques to reduce the likelihood of hallucinations. By following these tips, you can (hopefully) improve the accuracy of your AI applications.

在这篇文章中，我们将探讨几种简单的方法来降低幻觉的可能性。通过遵循这些建议，你（有望）能提高你的人工智能应用程序的准确性。

There are multiple types of hallucinations:

幻觉有多种类型：

Intrinsic hallucinations: the LLM’s response contradicts the user-provided context. This is when the response is verifiably wrong withing the current context.

内在幻觉：大语言模型的回应与用户提供的上下文相矛盾。这指的是在当前上下文中，回应存在可验证的错误。

Extrinsic hallucinations: the LLM’s response cannot be verified using the user-provided context. This is when the response may or may not be wrong but we have no way of confirming that using the current context.

外在幻觉：大语言模型的响应无法通过用户提供的上下文来验证。这种情况下，响应可能正确也可能错误，但我们无法利用当前上下文进行确认。

Incoherent hallucinations: the LLM’s response does not answer the question or does not make sense. This is when the LLM is unable to follow the instructions.

不连贯的幻觉：大语言模型的回应没有回答问题或毫无意义。这种情况发生在大语言模型无法遵循指令时。

In this post, we will target all the types mentioned above.

在本文中，我们将针对上述所有类型进行探讨。

We will list out a set of tips and tricks that work in different ways in reducing hallucinations.

我们将列出一系列以不同方式有助于减少幻觉的技巧和窍门。


Tip 1: Use Grounding 技巧1：使用锚定


Grounding is using in-domain relevant additional context in the input of the LLM when asking it to do a task. This gives the LLM the information it needs to correctly answer the question and reduces the likelihood of a hallucination. This is one the reason we use Retrieval augmented generation (RAG).

锚定是指在让大语言模型执行任务时，在其输入中使用与领域相关的额外上下文。这为大语言模型提供了正确回答问题所需的信息，并降低了产生幻觉的可能性。这也是我们使用检索增强生成（RAG）的原因之一。

For example asking the LLM a math question OR asking it the same question while providing it with relevant sections of a math book will yield different results, with the second option being more likely to be right.

例如，向大语言模型提出一个数学问题，或者在提出相同问题的同时提供一本数学书中的相关章节，会得到不同的结果，而第二种选择更有可能是正确的。

Here is an example of such implementation in one of my previous tutorials where I provide document-extracted context when asking a question:

以下是我之前一个教程中此类实现的示例，在该示例中，我在提问时提供了从文档中提取的上下文：

Build a Document AI pipeline for ANY type of PDF With Gemini

使用Gemini构建适用于任何类型PDF的文档AI管道

Tables, Images, figures or handwriting are not problem anymore ! Full Code provided.

表格、图像、图表或手写内容不再是问题！提供完整代码。

towardsdatascience.com 

Tip 2: Use structured outputs 技巧2：使用结构化输出

Using structured outputs means forcing the LLM to output valid JSON or YAML text. This will allow you to reduce the useless ramblings and get “straight-to-the-point” answers about what you need from the LLM. It also will help with the next tips as it makes the LLM responses easier to verify.

使用结构化输出意味着迫使大语言模型输出有效的JSON或YAML文本。这能让你减少无用的冗长表述，从大语言模型那里获得关于你需求的“直击要点”的答案。这也有助于后续的技巧实施，因为它使大语言模型的响应更易于验证。


```
import json

import google.generativeai as genai
from pydantic import BaseModel, Field

from document_ai_agents.schema_utils import prepare_schema_for_gemini


class Answer(BaseModel):
    answer: str = Field(..., description="Your Answer.")


model = genai.GenerativeModel("gemini-1.5-flash-002")

answer_schema = prepare_schema_for_gemini(Answer)


question = "List all the reasons why LLM hallucinate"

context = (
    "LLM hallucination refers to the phenomenon where large language models generate plausible-sounding but"
    " factually incorrect or nonsensical information. This can occur due to various factors, including biases"
    " in the training data, the inherent limitations of the model's understanding of the real world, and the "
    "model's tendency to prioritize fluency and coherence over accuracy."
)

messages = (
    [context]
    + [
        f"Answer this question: {question}",
    ]
    + [
        f"Use this schema for your answer: {answer_schema}",
    ]
)

response = model.generate_content(
    messages,
    generation_config={
        "response_mime_type": "application/json",
        "response_schema": answer_schema,
        "temperature": 0.0,
    },
)

response = Answer(**json.loads(response.text))

print(f"{response.answer=}")
```


Where “prepare_schema_for_gemini” is a utility function that prepares the schema to match Gemini’s weird requirements. You can find its definition here: code.

其中，“prepare_schema_for_gemini”是一个实用工具函数，用于调整架构以满足Gemini的特殊要求。你可以在这里找到它的定义：代码。

This code defines a Pydantic schema and sends this schema as part of the query in the field “response_schema”. This forces the LLM to follow this schema in its response and makes it easier to parse its output.

这段代码定义了一个Pydantic模式，并将该模式作为查询的一部分发送到“response_schema”字段中。这会迫使大语言模型在其响应中遵循该模式，从而使其输出更易于解析。



Tip 3: Use chain of thoughts and better prompting

技巧3：使用思维链和更优的提示词

Sometimes, giving the LLM the space to work out its response, before committing to a final answer, can help produce better quality responses. This technique is called Chain-of-thoughts and is widely used as it is effective and very easy to implement.

有时，让大语言模型在给出最终答案之前有空间去构思回应，有助于生成更高质量的回答。这种技术被称为思维链，因其有效且易于实施而被广泛使用。

We can also explicitly ask the LLM to answer with “N/A” if it can’t find enough context to produce a quality response. This will give it an easy way out instead of trying to respond to questions it has no answer to.

我们也可以明确要求大语言模型，如果找不到足够的上下文来生成高质量的回答，就用“N/A”来回应。这样一来，对于那些它无法回答的问题，它就有了一个简单的回避方式，而不必勉强作答。

For example, lets look into this simple question and context:

例如，让我们来看看这个简单的问题和背景：

Context 语境



Thomas Jefferson (April 13 [O.S. April 2], 1743 — July 4, 1826) was an American statesman, planter, diplomat, lawyer, architect, philosopher, and Founding Father who served as the third president of the United States from 1801 to 1809.[6] He was the primary author of the Declaration of Independence. Following the American Revolutionary War and before becoming president in 1801, Jefferson was the nation’s first U.S. secretary of state under George Washington and then the nation’s second vice president under John Adams. Jefferson was a leading proponent of democracy, republicanism, and natural rights, and he produced formative documents and decisions at the state, national, and international levels. (Source: Wikipedia)

托马斯·杰斐逊（1743年4月13日[旧历4月2日]—1826年7月4日）是美国政治家、种植园主、外交官、律师、建筑师、哲学家，也是美国开国元勋之一，1801年至1809年担任美国第三任总统。[6]他是《独立宣言》的主要起草人。美国独立战争后，在1801年成为总统之前，杰斐逊在乔治·华盛顿手下担任美国第一任国务卿，之后在约翰·亚当斯手下担任美国第二任副总统。杰斐逊是民主、共和主义和自然权利的主要倡导者，他在州、国家和国际层面制定了具有 formative意义的文件和决策。（来源：维基百科）

Question 问题

What year did davis jefferson die?

戴维斯·杰斐逊是哪一年去世的？

A naive approach yields: 一种简单的方法得出：

Response 回应

answer=’1826' answer=’1826

Which is obviously false as Jefferson Davis is not even mentioned in the context at all. It was Thomas Jefferson that died in 1826.

这显然是错误的，因为上下文根本没有提到杰斐逊·戴维斯。1826年去世的是托马斯·杰斐逊。

If we change the schema of the response to use chain-of-thoughts to:

如果我们将响应的模式改为使用思维链：

```
class AnswerChainOfThoughts(BaseModel):
    rationale: str = Field(
        ...,
        description="Justification of your answer.",
    )
    answer: str = Field(
        ..., description="Your Answer. Answer with 'N/A' if answer is not found"
    )
```
We are also adding more details about what we expect as output when the question is not answerable using the context “Answer with ‘N/A’ if answer is not found”

我们还在补充更多细节，说明当无法利用上下文回答问题时，我们期望的输出是什么——“如果找不到答案，请用‘N/A’作答”。

With this new approach, we get the following rationale (remember, chain-of-thought):

通过这种新方法，我们得到了以下推理依据（请记住，这是思维链）：

The provided text discusses Thomas Jefferson, not Jefferson Davis. No information about the death of Jefferson Davis is included.

所提供的文本讨论的是托马斯·杰斐逊，而非杰斐逊·戴维斯。其中没有包含任何关于杰斐逊·戴维斯死亡的信息。


And the final answer: 最后的答案：

answer=’N/A’ answer=’不适用’

Great ! But can we use a more general approach to hallucination detection?

太棒了！但是我们能不能用一种更通用的方法来检测幻觉呢？

We can, with Agents! 我们可以做到，借助智能体！

Tip 4: Use an Agentic approach 技巧4：采用智能体方法

We will build a simple agent that implements a three-step process:

我们将构建一个简单的智能体，它将实现一个三步流程：

The first step is to include the context and ask the question to the LLM in order to get the first candidate response and the relevant context that it had used for its answer.


第一步是提供上下文并向大语言模型提问，以获取第一个候选回复及其回答所使用的相关上下文。

The second step is to reformulate the question and the first candidate response as a declarative statement.

第二步是将问题和第一个候选回答重新表述为一个陈述句。

The third step is to ask the LLM to verify whether or not the relevant context entails the candidate response. It is called “Self-verification”: https://arxiv.org/pdf/2212.09561

第三步是让大语言模型验证相关语境是否包含候选回答。这被称为“自我验证”：https://arxiv.org/pdf/2212.09561

In order to implement this, we define three nodes in LangGraph. The first node will ask the question while including the context, the second node will reformulate it using the LLM and the third node will check the entailment of the statement in relation to the input context.

为了实现这一点，我们在LangGraph中定义了三个节点。第一个节点会在包含上下文的情况下提出问题，第二个节点会使用大语言模型对其进行重新表述，第三个节点会检查该陈述与输入上下文之间的蕴含关系。

The first node can be defined as follows:

第一个节点可以定义如下：


```
    def answer_question(self, state: DocumentQAState):
        logger.info(f"Responding to question '{state.question}'")
        assert (
            state.pages_as_base64_jpeg_images or state.pages_as_text
        ), "Input text or images"
        messages = (
            [
                {"mime_type": "image/jpeg", "data": base64_jpeg}
                for base64_jpeg in state.pages_as_base64_jpeg_images
            ]
            + state.pages_as_text
            + [
                f"Answer this question: {state.question}",
            ]
            + [
                f"Use this schema for your answer: {self.answer_cot_schema}",
            ]
        )

        response = self.model.generate_content(
            messages,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": self.answer_cot_schema,
                "temperature": 0.0,
            },
        )

        answer_cot = AnswerChainOfThoughts(**json.loads(response.text))

        return {"answer_cot": answer_cot}
```
And the second one as: 第二个是：


```
    def reformulate_answer(self, state: DocumentQAState):
        logger.info("Reformulating answer")
        if state.answer_cot.answer == "N/A":
            return

        messages = [
            {
                "role": "user",
                "parts": [
                    {
                        "text": "Reformulate this question and its answer as a single assertion."
                    },
                    {"text": f"Question: {state.question}"},
                    {"text": f"Answer: {state.answer_cot.answer}"},
                ]
                + [
                    {
                        "text": f"Use this schema for your answer: {self.declarative_answer_schema}"
                    }
                ],
            }
        ]

        response = self.model.generate_content(
            messages,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": self.declarative_answer_schema,
                "temperature": 0.0,
            },
        )

        answer_reformulation = AnswerReformulation(**json.loads(response.text))

        return {"answer_reformulation": answer_reformulation}
```
The third one as: 第三个是：

```
    def verify_answer(self, state: DocumentQAState):
        logger.info(f"Verifying answer '{state.answer_cot.answer}'")
        if state.answer_cot.answer == "N/A":
            return
        messages = [
            {
                "role": "user",
                "parts": [
                    {
                        "text": "Analyse the following context and the assertion and decide whether the context "
                        "entails the assertion or not."
                    },
                    {"text": f"Context: {state.answer_cot.relevant_context}"},
                    {
                        "text": f"Assertion: {state.answer_reformulation.declarative_answer}"
                    },
                    {
                        "text": f"Use this schema for your answer: {self.verification_cot_schema}. Be Factual."
                    },
                ],
            }
        ]

        response = self.model.generate_content(
            messages,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": self.verification_cot_schema,
                "temperature": 0.0,
            },
        )

        verification_cot = VerificationChainOfThoughts(**json.loads(response.text))

        return {"verification_cot": verification_cot}
```

Full code in https://github.com/CVxTz/document_ai_agents

完整代码见https://github.com/CVxTz/document_ai_agents

Notice how each node uses its own schema for structured output and its own prompt. This is possible due to the flexibility of both Gemini’s API and LangGraph.

注意每个节点如何使用自己的模式来生成结构化输出，并使用自己的提示词。这得益于Gemini API和LangGraph两者的灵活性。

Lets work through this code using the same example as above ➡️

让我们使用上面相同的示例来逐步讲解这段代码 ➡️

(Note: we are not using chain-of-thought on the first prompt so that the verification gets triggered for our tests.)

（注：我们在第一个提示词中没有使用思维链，以便在测试中触发验证。）

Context 语境

Thomas Jefferson (April 13 [O.S. April 2], 1743 — July 4, 1826) was an American statesman, planter, diplomat, lawyer, architect, philosopher, and Founding Father who served as the third president of the United States from 1801 to 1809.[6] He was the primary author of the Declaration of Independence. Following the American Revolutionary War and before becoming president in 1801, Jefferson was the nation’s first U.S. secretary of state under George Washington and then the nation’s second vice president under John Adams. Jefferson was a leading proponent of democracy, republicanism, and natural rights, and he produced formative documents and decisions at the state, national, and international levels. (Source: Wikipedia)

托马斯·杰斐逊（1743年4月13日[旧历4月2日]—1826年7月4日）是美国政治家、种植园主、外交官、律师、建筑师、哲学家，也是美国开国元勋之一，1801年至1809年担任美国第三任总统。[6]他是《独立宣言》的主要起草人。美国独立战争后，在1801年成为总统之前，杰斐逊在乔治·华盛顿手下担任美国第一任国务卿，之后在约翰·亚当斯手下担任美国第二任副总统。杰斐逊是民主、共和主义和自然权利的主要倡导者，他在州、国家和国际层面制定了具有 formative意义的文件和决策。（来源：维基百科）

Question 问题

What year did davis jefferson die?

戴维斯·杰斐逊是哪一年去世的？

First node result (First answer): 第一个节点结果（第一个答案）：

relevant_context=’Thomas Jefferson (April 13 [O.S. April 2], 1743 — July 4, 1826) was an American statesman, planter, diplomat, lawyer, architect, philosopher, and Founding Father who served as the third president of the United States from 1801 to 1809.’

relevant_context=’托马斯·杰斐逊（1743年4月13日[旧历4月2日]—1826年7月4日）是美国政治家、种植园主、外交官、律师、建筑师、哲学家和开国元勋，1801年至1809年担任美国第三任总统。’

answer=’1826' answer=’1826

Second node result (Answer Reformulation):

第二个节点结果（答案重构）：

declarative_answer=’Davis Jefferson died in 1826'

declarative_answer=’戴维斯·杰斐逊于1826年去世

Third node result (Verification): 第三个节点结果（验证）：

rationale=’The context states that Thomas Jefferson died in 1826. The assertion states that Davis Jefferson died in 1826. The context does not mention Davis Jefferson, only Thomas Jefferson.’

理由=‘上下文表明托马斯·杰斐逊于1826年去世。断言称戴维斯·杰斐逊于1826年去世。上下文未提及戴维斯·杰斐逊，只提到了托马斯·杰斐逊。’

entailment=’No’ 蕴含=’否’

So the verification step rejected (No entailment between the two) the initial answer. We can now avoid returning a hallucination to the user.

因此，验证步骤拒绝了初始答案（两者之间没有蕴含关系）。我们现在可以避免向用户返回幻觉内容了。

Bonus Tip : Use stronger models 额外提示：使用更强大的模型

This tip is not always easy to apply due to budget or latency limitations but you should know that stronger LLMs are less prone to hallucination. So, if possible, go for a more powerful LLM for your most sensitive use cases. You can check a benchmark of hallucinations here: https://github.com/vectara/hallucination-leaderboard. We can see that the top models in this benchmark (least hallucinations) also ranks at the top of conventional NLP leader boards.

由于预算或延迟限制，这条建议并非总能轻易实施，但你应当知道，更强大的大语言模型（LLM）不太容易产生幻觉。因此，在可能的情况下，对于那些最敏感的使用场景，要选择更强大的大语言模型。你可以在这里查看幻觉问题的基准测试：https://github.com/vectara/hallucination-leaderboard。我们可以发现，在该基准测试中排名靠前的模型（幻觉最少）在传统的自然语言处理（NLP）排行榜上也位居前列。



Conclusion 结论

In this tutorial, we explored strategies to improve the reliability of LLM outputs by reducing the hallucination rate. The main recommendations include careful formatting and prompting to guide LLM calls and using a workflow based approach where Agents are designed to verify their own answers.

在本教程中，我们探讨了通过降低幻觉率来提高大语言模型输出可靠性的策略。主要建议包括精心设计格式和提示词以引导大语言模型调用，以及采用基于工作流的方法，即设计智能体来验证自己的答案。

This involves multiple steps: 这涉及多个步骤：

Retrieving the exact context elements used by the LLM to generate the answer.

检索大型语言模型生成答案时所使用的确切上下文元素。


Reformulating the answer for easier verification (In declarative form).

将答案重新表述以便于验证（采用陈述形式）。


Instructing the LLM to check for consistency between the context and the reformulated answer.

指示大语言模型检查上下文与重新表述的答案之间的一致性。

While all these tips can significantly improve accuracy, you should remember that no method is foolproof. There’s always a risk of rejecting valid answers if the LLM is overly conservative during verification or missing real hallucination cases. Therefore, rigorous evaluation of your specific LLM workflows is still essential.

虽然所有这些技巧都能显著提高准确性，但你应该记住，没有哪种方法是万无一失的。如果大语言模型在验证过程中过于保守，就可能会拒绝有效的答案；或者可能会遗漏真正的幻觉案例，这些风险始终存在。因此，对特定的大语言模型工作流程进行严格评估仍然至关重要。

Full code in https://github.com/CVxTz/document_ai_agents

完整代码见https://github.com/CVxTz/document_ai_agents




