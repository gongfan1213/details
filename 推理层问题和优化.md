把应用的状态转换成为连续向量，直接嵌入到模型的注意力层当中，实现隐士检索

推理 输入问题和输出答案之间的中间步骤，生成token，

任何可以通过布尔电路解决的问题，都可以通过生成中间的token来恒定大小的transformer模型来解决的


逻辑电路的大小决定了解决问题的能力，

语言模型已经具备了推理的能力，关键在于解码的过程

链式推理解码：如果我们多考虑一些候选的答案，但是如果我们多考虑一些候选的答案，而不是只选择一个最可能的候选答案，模型就能产生一个更佳正确的答案。

超越贪婪解码，检查更多的生成候选，

选择最后答案置信度最高的候选。

逐步思考：

不需要找到类似的实例，只需要说让我们一步步思考就会有奇迹出现的但是表现比少量实例差的多的

sft泛化能力有限

1.解决人类标注错误的问题，机器生辰好的数据可能优于人类构建的数据

openai response API

<img width="919" height="777" alt="image" src="https://github.com/user-attachments/assets/dd1de81d-570b-4051-8e23-0a550d25603a" />

一旦更好的模型生成了响应或者训练数据，模型就可以自我改进

直接优化我们想要的东西，

如果我们想要构建一个用于推理的模型，或者只是一般用于生成有趣1的内容的，我们就u需要优化衡量生成质量的指标，一旦你有了一个度量的标准，我们所需要做的计算该度量的标准的梯度并且进行反响的传播

假设模型是一个先验的模型，我们需要最大化该指标的期望值，我们需要进行采样来计算期望值，策略梯度的原因。


真正的扩展只有两个过程，一个是学习，另外一个是搜索的，学习是可以扩展的。

我们来思考一下 LLM 的解码过程。给定问题和生成器推理，然后输出最终答案，接着是通过网格解码关键找到的响应，那么关键点就是匹配概率。

来说，需要选择概率最大的答案。所以它们没有对齐，我们只需要再进一步。如果我们生成推理过程，我们应该有一些整体推理过程来找出最终答案在机器学习方面的概率，这被称为边缘化。所有这些原因实际上本质上都只是潜在变量。如果我们刚开始接触机器学习，实际上就会知道这个和可以通过采样来计算。


发生了什么呢？当我告诉模型回忆相关问题时，模型找到了一个相关但不同的问题。其实，这是一个与当前问题相关的问题，涉及计算坐标平面上两点之间的距离，并给出了公式。然后，模型说：「哦，我现在知道如何计算距离了，接着我就可以计算面积。」这个例子展示了检索在推理中的重要性。

另一个例子是「后退一步」的方法。在解决问题之前，我们给模型提供了一些简短的例子，让它理解如何抽象化思考。例如，在解决实际问题之前，我们可以让模型「后退一步」，思考更抽象的原则，然后再应用到实际问题中。这就是检索在推理中的作用。

RL fnetuning的优势在于它的泛化能力很好的，




推理api staleless
