                      38+35+15+24+22+38=171

All my royalties will be donated to Save the Children

# Part 1

[**Introduction	1**](?tab=t.0#heading=h.rhe5fymgnap)

[**The Dawn of a New Architecture	1**](?tab=t.0#heading=h.70rqbxd9u855)

[**The Core Transformer Architecture: An Overview	2**](?tab=t.0#heading=h.2mgaqkx736ah)

[The Encoder in Detail	3](?tab=t.0#heading=h.4dh545unq8fc)

[Inside an Encoder Layer:	4](?tab=t.0#heading=h.gscyprjjbyus)

[The Decoder in Detail	4](?tab=t.0#heading=h.cgj2u1i3jy6j)

[Inside a Decoder Layer:	5](?tab=t.0#heading=h.cfosm1ntqx0x)

[**Attention is All You Need: The Core Mechanism	5**](?tab=t.0#heading=h.r4x856u89x7n)

[The Inputs: Queries, Keys, and Values (Q, K, V)	6](?tab=t.0#heading=h.681kxx1t8v0o)

[The Attention Formula	6](?tab=t.0#heading=h.m0hvp5229k5c)

[Multi-Head Attention: Seeing in Parallel	7](?tab=t.0#heading=h.7d4ai8354mem)

[Types of Attention in the Transformer	8](?tab=t.0#heading=h.7obn4xnxmtn0)

[Computational Complexity and Benefits of Attention	9](?tab=t.0#heading=h.6dej52kr48u1)

[**The Transformer Lifecycle	11**](?tab=t.0#heading=h.lofpmoy4xxp8)

[Pretraining	11](?tab=t.0#heading=h.bth6fjia9op3)

[Fine-Tuning	13](?tab=t.0#heading=h.ge1ucpb5ecja)

[Alignment	14](?tab=t.0#heading=h.rs7dqk103i4y)

[**Modern Transformers	16**](?tab=t.0#heading=h.w06al4do7xbq)

[Context Management: The Context Window	16](?tab=t.0#heading=h.tiexkis77p6u)

[The Challenge of Long Sequences	17](?tab=t.0#heading=h.1csxsb5x6xcz)

[Techniques for Extending Effective Context	18](?tab=t.0#heading=h.gkptzg27lbg5)

[Prompting and Inference: The Art of Talking to LLMs	20](?tab=t.0#heading=h.vztkthodf8qq)

[The Inference Process: From Probabilities to Text	21](?tab=t.0#heading=h.do0jw22oqvl1)

[The Rise of Decoder-Only Architectures	24](?tab=t.0#heading=h.c75tnqq6zyc4)

[Mixture-of-Experts (MoE) Models	25](?tab=t.0#heading=h.tc0mr34rlgqc)

[Multimodality: Beyond Text	27](?tab=t.0#heading=h.2j30dv1ofvhr)

[The Problem of Hallucination	28](?tab=t.0#heading=h.sd1vs1o618xp)

[Interpretability and Mechanistic Interpretability	29](?tab=t.0#heading=h.3vdcith8rfx4)

[The Evolving Story of Scaling Laws: From Training to "Thinking"	30](?tab=t.0#heading=h.t9q84rvrh7ks)

[The Foundational Laws of Training	31](?tab=t.0#heading=h.1s73dhrvc80t)

[The New Frontier: Test-Time Compute Scaling	31](?tab=t.0#heading=h.rhz69th73ai2)

[The Unified View: A New, More Complex Trade-Off	32](?tab=t.0#heading=h.62bie0mzhryi)

[**Alternative Architectures	32**](?tab=t.0#heading=h.k84c6kh9f3v8)

[State-Space Models (Mamba and beyond)	32](?tab=t.0#heading=h.e2wqe5np88uh)

[Diffusion Transformers (DiT)	34](?tab=t.0#heading=h.v0q90kty8adx)

[**The Path to Artificial General Intelligence (AGI)	35**](?tab=t.0#heading=h.ojo6bay9j4i1)

[**Handson	36**](?tab=t.0#heading=h.vbhosfn91x4y)

[**Key Takeaways	36**](?tab=t.0#heading=h.algv5z2livr2)

[**Conclusion	38**](?tab=t.0#heading=h.6zlze5rdimo)

[**Glossary of Key Terms	38**](?tab=t.0#heading=h.rvxi6x7czogx)

# Part 2

[**Introduction	1**](?tab=t.atn7wq2c958o#heading=h.n7ftxe1kczw)

[**The Architectural Foundations of Modern AI Reasoning	3**](?tab=t.atn7wq2c958o#heading=h.8gjc2ppjep0c)

[Inference-Time Scaling: Deeper Thinking on Demand	3](?tab=t.atn7wq2c958o#heading=h.vrwxrd72ygu4)

[Chain of Thought (CoT)	3](?tab=t.atn7wq2c958o#heading=h.qf3tnm1cbbcr)

[Few shots and CoT	7](?tab=t.atn7wq2c958o#heading=h.mo7rnsd2jrn9)

[Tree of Thoughts: From Linear Steps to Exploratory Search	9](?tab=t.atn7wq2c958o#heading=h.c2execm09gag)

[The Alignment Engine	11](?tab=t.atn7wq2c958o#heading=h.nztul2g0qbej)

[Reinforcement Learning from Human Feedback (RLHF)	11](?tab=t.atn7wq2c958o#heading=h.h643a113wec9)

[Reinforcement Learning from AI Feedback (RLAIF)	13](?tab=t.atn7wq2c958o#heading=h.53e0u25ibvgf)

[Pure Reinforcement Learning (RL)	14](?tab=t.atn7wq2c958o#heading=h.5lv6ciebp0fn)

[Key RL Algorithms in Alignment (PPO and DPO)	14](?tab=t.atn7wq2c958o#heading=h.4zyq9oojpxim)

[Supervised Fine-Tuning \+ Reinforcement Learning (SFT \+ RL)	15](?tab=t.atn7wq2c958o#heading=h.hcw9wbf9uuwe)

[Pure SFT and Distillation	18](?tab=t.atn7wq2c958o#heading=h.mi2gchu5cpke)

[Thinking on Demand: The Rise of Test-Time Computation and Adaptive Inference	20](?tab=t.atn7wq2c958o#heading=h.dq1ky1r5p0vb)

[The Scaling Dilemma: Sparse Mixture-of-Experts and Computational Efficiency	21](?tab=t.atn7wq2c958o#heading=h.ee6gt7m9bg9u)

[**A Comparative Analysis of Leading Models	24**](?tab=t.atn7wq2c958o#heading=h.g2e53hh64cfu)

[The Titans of Industry (Proprietary Models)	24](?tab=t.atn7wq2c958o#heading=h.bldhg3d7sfta)

[The Open-Source Insurgency	25](?tab=t.atn7wq2c958o#heading=h.dzx86w9i8oc7)

[**Benchmarking Reasoning Capabilities	28**](?tab=t.atn7wq2c958o#heading=h.yh7yixfcvhq4)

[The Established Canon and Its Limitations	28](?tab=t.atn7wq2c958o#heading=h.cs9r2feuwtez)

[The 2025 Leaderboard	28](?tab=t.atn7wq2c958o#heading=h.jc1jwdho80tu)

[**Future Directions and Implications	29**](?tab=t.atn7wq2c958o#heading=h.ch76tae05pvq)

[Next-Generation Reasoning Paradigms	29](?tab=t.atn7wq2c958o#heading=h.jiiaurwdsher)

[Neuro-Symbolic AI: Bridging Logic and Intuition	30](?tab=t.atn7wq2c958o#heading=h.ceunzpes4wzz)

[Dynamic Reasoning Frameworks	31](?tab=t.atn7wq2c958o#heading=h.cttp2l5r6nf2)

[Adversarial Self-Critique	32](?tab=t.atn7wq2c958o#heading=h.p6zv6ce0mzha)

[**Key Takeaways	33**](?tab=t.atn7wq2c958o#heading=h.xsg8fpwpthp3)

[**Conclusion	34**](?tab=t.atn7wq2c958o#heading=h.os5gujcqlgfk)

[**References	35**](?tab=t.atn7wq2c958o#heading=h.e433k9sbrqfg)

# Part 3

[**Introduction	1**](?tab=t.35w47fciyb7s#heading=h.y154sashuimp)

[Code Breakdown	1](?tab=t.35w47fciyb7s#heading=h.x96e7sqsq9k5)

[Initialization and Hyperparameters	1](?tab=t.35w47fciyb7s#heading=h.20qm3mye3ogq)

[Model Definition	2](?tab=t.35w47fciyb7s#heading=h.5oy69uhb8q8i)

[Multi Headed Attention	3](?tab=t.35w47fciyb7s#heading=h.jhy9atv5nmo4)

[Transformer Block	4](?tab=t.35w47fciyb7s#heading=h.6coto9pyl3oj)

[Attention Language Model	5](?tab=t.35w47fciyb7s#heading=h.4mg43hotub2i)

[Training logic	6](?tab=t.35w47fciyb7s#heading=h.c9q42i93qct3)

[Evaluate Logic	7](?tab=t.35w47fciyb7s#heading=h.smn5xercdjir)

[Data and Generation Helpers	8](?tab=t.35w47fciyb7s#heading=h.tuc0bv6v2w5n)

[**Main execution	10**](?tab=t.35w47fciyb7s#heading=h.ismwjxjxun3r)

[**Key Takeaways	14**](?tab=t.35w47fciyb7s#heading=h.rlum16vf33mi)

[**Conclusion	15**](?tab=t.35w47fciyb7s#heading=h.lpzhb0kiv8uf)

# Part 4

[**Introduction	1**](?tab=t.yb1ae938b92f#heading=h.58jfllcd6aqh)

[Mixture of Experts (MoE)	1](?tab=t.yb1ae938b92f#heading=h.82ctaxq2fj40)

[Grouped-Query Attention (GQA)	2](?tab=t.yb1ae938b92f#heading=h.3ov2f13wt0r5)

[Rotary Position Embeddings (RoPE)	3](?tab=t.yb1ae938b92f#heading=h.7rygwb9q943l)

[RMSNorm (Root Mean Square Normalization)	4](?tab=t.yb1ae938b92f#heading=h.rglyjbwrnllg)

[SwiGLU (Swish-Gated Linear Unit)	4](?tab=t.yb1ae938b92f#heading=h.6b3mxpchbk45)

[KV Caching (Key-Value Caching)	5](?tab=t.yb1ae938b92f#heading=h.y3ciixge14qd)

[**Code Breakdown	6**](?tab=t.yb1ae938b92f#heading=h.x7876u2qe1u3)

[Model Architecture	6](?tab=t.yb1ae938b92f#heading=h.vcw2bupf9oi5)

[Tokenizer	16](?tab=t.yb1ae938b92f#heading=h.u6y0n0dvgcg7)

[Weight Loading	17](?tab=t.yb1ae938b92f#heading=h.m4797zc5umke)

[Text Generation	20](?tab=t.yb1ae938b92f#heading=h.my9g7spzckpn)

[Main Execution	21](?tab=t.yb1ae938b92f#heading=h.v87lg160rulj)

[**Key Takeaways	23**](?tab=t.yb1ae938b92f#heading=h.tuea2gb7yne1)

[**Conclusions	24**](?tab=t.yb1ae938b92f#heading=h.yvp68ix4jz5)

# Part 5

[**Introduction: An Era of Refinement	1**](?tab=t.q9eydfqd5d2i#heading=h.sx2dluxnodct)

[**Key Architectural Innovations of the Modern Era	1**](?tab=t.q9eydfqd5d2i#heading=h.nydc9qkouy95)

[The Rise of Sparse Models: Mixture-of-Experts (MoE)	2](?tab=t.q9eydfqd5d2i#heading=h.fs5g2gmeiz53)

[Evolving the Attention Mechanism: Beyond Multi-Head	2](?tab=t.q9eydfqd5d2i#heading=h.ub8if8nyvt19)

[Grouped-Query Attention (GQA)	2](?tab=t.q9eydfqd5d2i#heading=h.6jcuwu306dzc)

[Multi-Head Latent Attention (MLA)	3](?tab=t.q9eydfqd5d2i#heading=h.a2af2ky7953m)

[Sliding Window Attention	4](?tab=t.q9eydfqd5d2i#heading=h.utvq9oaoj5g4)

[The Subtle Art of Normalization and Positional Signals	5](?tab=t.q9eydfqd5d2i#heading=h.n17tr83g36r3)

[Normalization: The Art of Stability	5](?tab=t.q9eydfqd5d2i#heading=h.nlb55ebosqq7)

[No Positional Embeddings (NoPE)	6](?tab=t.q9eydfqd5d2i#heading=h.grl3gsc1dxby)

[Sliding Attention Window	7](?tab=t.q9eydfqd5d2i#heading=h.54pftjbbrn1q)

[**A Tour of 2025's Flagship Architectures	8**](?tab=t.q9eydfqd5d2i#heading=h.37p8l36ka09a)

[DeepSeek-V3	8](?tab=t.q9eydfqd5d2i#heading=h.58mw25l8tqmk)

[OLMo 2	9](?tab=t.q9eydfqd5d2i#heading=h.vqbpu0r1spwp)

[Gemma 3	10](?tab=t.q9eydfqd5d2i#heading=h.542hg27ope88)

[Llama 4: Mainstreaming the Mixture-of-Experts	11](?tab=t.q9eydfqd5d2i#heading=h.mp625c1mgrnu)

[Qwen3: The Hallmark of Versatility	13](?tab=t.q9eydfqd5d2i#heading=h.4k4yu0b1mm5)

[SmolLM3 and the Frontier of Positional Information	15](?tab=t.q9eydfqd5d2i#heading=h.c0dyg4q83ipq)

[GTP-OSS: the OpenAI Open Source take	15](?tab=t.q9eydfqd5d2i#heading=h.9zxhdbw0erlq)

[Kimi2: the new model from China	18](?tab=t.q9eydfqd5d2i#heading=h.ma5jhdo4bo4g)

[**Key Takeaways	20**](?tab=t.q9eydfqd5d2i#heading=h.c8qw2joirin6)

[**Conclusion	21**](?tab=t.q9eydfqd5d2i#heading=h.oa8t7j7z09zg)

# Part 6

[**Introduction	1**](?tab=t.tnwjtxkmyfq0#heading=h.ydx5smy7wsa2)

[**Gemma Model	1**](?tab=t.tnwjtxkmyfq0#heading=h.4pldaqyumgtn)

[gemma/config.py	1](?tab=t.tnwjtxkmyfq0#heading=h.p13hudbb4tzi)

[gemma/gemma3\_model.py	2](?tab=t.tnwjtxkmyfq0#heading=h.nkxg1jmaari9)

[gemma/gemma3\_preprocessor.py	12](?tab=t.tnwjtxkmyfq0#heading=h.mmqr4el9wt47)

[gemma/model.py	12](?tab=t.tnwjtxkmyfq0#heading=h.fi48u9comm9x)

[gemma/model\_xla.py	31](?tab=t.tnwjtxkmyfq0#heading=h.v7b2a36x7fb9)

[gemma/tokenizer.py	36](?tab=t.tnwjtxkmyfq0#heading=h.k4f0wvfm9odp)

[gemma/xla\_model\_parallel.py	36](?tab=t.tnwjtxkmyfq0#heading=h.r4f4kbcynbud)

[**Conclusion	38**](?tab=t.tnwjtxkmyfq0#heading=h.1bwhs2w49vsu)

