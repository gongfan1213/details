|模型名称|主要特点|中文领域表现|适用场景|
| ---- | ---- | ---- | ---- |
|Qwen3-Embedding-8B|8B参数量，多语言SOTA，支持100+语言，长文本理解，指令支持，可自定义维度|目前MTEB多语言榜单第一，中文性能极佳|对性能要求极高的通用中文语任务，RAG，需要灵活维度和指令控制的场景|
|BGE-M3|多语言（100+），多功能（稠密/多向量/稀疏检索），多粒度（8192 token），指令微调|在中文RAG和通用检索任务中表现出色|需要处理多语言、长文本、以多种检索方式（混合检索）的中文RAG和信息检索系统|
|BCEmbedding|双语（中/英）和跨语言能力强，RAG基石，双编码器 + 交叉编码器，领域适应性强，无指令|在中英文双语和跨语言任务上表现优异，尤其适合RAG|中英文混合或跨语言场景，RAG强调高效和精确检索|
|BGE-Large-Zh|中文专属优化，1024维，512 token最大长度|中文语义理解和检索的强大选择|专注于中文的语义搜索、相似计算、文本分类等任务|
|M3E-base|中英文混合训练，轻量级（110M参数），高效推理，指令数据训练|中文文本分类和检索方面性能优异|资源受限但需要中英文混合处理的场景，快速推理、通用中文语义任务|
|Nomic Embed Text V1.5|多语言（100+），长序列（8192 token），Matryoshka灵活维度，多模态（与Vision对齐），任务类型优化，开源|在中文通用任务上有竞争力，但可能不如中文专属模型|需要处理长文本、多语言、以灵活维度选择的通用语义任务，多模态需求的场景|


1.对于外人来说几乎完全相同的查询，在领域专家看来有重大区别，通用嵌入模型可能会忽略细微差别

2.如果reranker模型没有经过专业术语，行业黑话或者缩略语的训练，可能会将相关的信息误判为不相关的

3.大语言模型可能对特定领域下的专业称呼，缩写或者超出其训练数据范围之外的知识产生认知混乱

## 4.2.2.1 确定业务领域 
构建评估数据集的第一步是确定业务范围，也就是先搞明白你数据集用在什么场景？比如：
- 法律问答助手？→ 构建法律类数据集 
- 金融资讯机器人？→ 构建金融类数据集 
- 企业内部文档助手？→ 构建公司知识库数据集 

💡注意：只有聚焦核心业务场景，构建的数据集才有意义。 

虽然第一步看起来很傻，但别不信，我还真见过有些大聪明瞎搞的，而且还是一流大厂。 

## 4.2.2.2 准备问题答案 
确定业务领域后，第二步我们需要准备问题和答案。样例如下： 

**代码块** 
```
{
  "question": "《喜剧之王》中柳飘飘一夜情后对尹天仇说了什么？", 
  "answer": "在电影《喜剧之王》中，一夜情之后，柳飘飘对尹天仇说：“你不会真的以为我喜欢你吧？大家都
``` 


准备问题和答案的方法通常有以下三种：

准备问题和答案的方法通常有以下三种：
1. **手动构造**。你需要跟业务专家合作，编写问题和答案，并确保内容清晰和权威，并标明出处
2. **寻找开源数据集**。可以参考一些开源的 QA 数据集，比如：

|数据集|适用领域|简介|
| ---- | ---- | ---- |
|Natural Questions|通用|来自 Google 搜索问题|
|HotpotQA|多跳问答|有支持文档和答案|
|SQUAD|阅读理解|标注精确答案片段|
|（此处因显示不全，原内容难完整呈现 ）|（显示不全 ）|金融文本问答对|
|（显示不全 ）|（显示不全 ）|来自 Reddit| 

这里也分享下我以前做医学模型时参考了一些数据集：

## 4.2.2.3 标注支持文档 

问题答案准备完毕后，第三步我们需要标注“支持文档”。 

该步骤是“RAG 黄金数据集”最关键的部分！ 

它最终体现在数据结构的 suppo 

```json
{
  "question": "《喜剧之王》中柳飘飘一夜情后对尹天仇说了什么？",
  "answer": "在电影《喜剧之王》中，一夜情之后，柳飘飘对尹天仇说：“你不会真的以为我喜欢你吧？大家都是成年”
  "supporting_docs": [
    "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’尹天仇听后愣住",
    "尹天仇从床上坐起，刚想说话，却被柳飘飘打断。她刻意用轻松的语气掩饰自己的复杂情绪。",
    "这个桥段展现了两人情感的错位，尹天仇认真投入，而柳飘飘则用防备和自嘲隐藏内心。"
  ],
  "doc_ids": ["scene_26", "scene_26_note", "analysis_emotion_26"],
  "metadata": {
    "movie": "喜剧之王",
    "characters": ["尹天仇", "柳飘飘"],
    "scene": "一夜情后",
    "emotion": "情感错位 / 冷漠掩饰",
    "difficulty": "较高"
  }
}
```
关于数据集的 doc_ids 和 metadata 字段，大家可以按照自己实际需要进行标注。 
自此，黄金评估数据集构建完毕。 


从这条数据样本我们可以看出：

- 数据结构完整：这条数据有问题、有答案、有出处、有出处索引，甚至还有元数据过滤

定义评估指标-》构建黄金评估数据集-〉检索评估-》生成评估-〉端到端评估

## 4.2.2.4 数据质量校验 
评估数据集构建完毕后，第四步进入数据集的质检阶段，该阶段往往需要多角色参与，比如客户、业务专家、客服、算法专家、大数据专家等，更多相关角色介入，更多视角关注，更容易尽早发现问题。 

数据集质检阶段需要做下面几件事：| 
- 检查问题的答案是否准确、规范？（简单说就是数据准不准确） 
- 根据查询是否可以推导出答案？（简单说就是数据逻辑是否合理） 
- 数据集是否完整、是否覆盖主要业务场景、是否支持不同类型问题？（简单说就是数据完不完整） 
- 是否存在冗余或含糊内容？（简单说就是数据干不干净） 

数据集质检一般做法是人工和大模型相结合的方式。如果觉得一个大模型不准，也可以考虑多个大模型一起评估，然后共识出结果。 

## 4.2.2.5 黄金数据举例 
构建黄金评估数据集的流程已基本介绍完毕。 
下面，我带大家再看下这条数据样本，我再解释下为什么我认为它是一条“黄金”数据： 

- 问题真实自然：它模拟了人性化提问，并非百科书式的提问，比如：“柳飘飘是谁”？ 
- 问题答案准确：它引用了电影关键词，解释情感动机，内容忠实。 
- 支持文档完整：支持文档就是答案出处。它选自电影对白和剧情描写，支持检索。 
- 可评估性强：可以检验系统是否检索到台词场景、是否生成忠实回答。 

当然，关于黄金数据内容，不同业务领域可能会有细微差别。 

这里非常欢迎大家各抒己见，结合实际工作和学习场景，在评论区或弹幕区表达自己的想法和观点。 

### 4.2.2.5 黄金数据扩展 
有时，当你构建出一条自认为“完美”的黄金数据后，你完全可以趁热打铁，继续顺着这条黄金数据的思路，扩展出更多黄金数据出来。 

比如我们的黄金数据是：“柳飘飘一夜情后对尹天仇说了什么？”。 

你完全可以继续构建下列场景，比如：
- 尹天仇在海岸边，向柳飘飘表白时说了什么？ 
- 柳飘飘在计程车中哭到崩溃时的动作和情绪？ 
- 后来面试……（内容显示不全 ） 

等等。通过这种技巧，你会发现所谓数据集构建，并非无中生有编造，而是顺理成章。 

这种扩展思路在实际业务场景中非常有用，甚至可以演进成上下文相关、甚至是推理型黄金数据。

### 4.2.3 构建数据集最佳实践 
最后，我再介绍下构建评估黄金数据集的一些其他技巧。 

#### 数量上 
可以先收集 10 - 50 条数据样本。每条样本包括：
- 问题 
- 标准答案 
- 2 - 3 条支持文档段落 
- 文档段落的索引 ID 
- 元数据 

有了这些数据样本，后续你就可以用来：
- 评估检索器是否命中支持文档 
- 评估大模型生成是否基于支持文档、是否偏离事实，是否贴合标准答案 

#### 数据样本选择上 
也有一些技巧。比如：
- ✅ 优先选择高频问题（比如 FAQ） 
- ✅ 问题尽量自然，符合真人口吻 
- ✅ 文档片段不要太长，这样可以避免不必要的噪音 
- ✅ 答案要能从支持文档中“推出”，不能脱离事实
× 尽量别选择“常识题”，而且要确保数据质量过关 

### 工具使用上 
最好走组合搭配路线。比如：
- ✅ 使用 Excel / Notion 手动标注小批量数据，这种轻量使用体验友好，没有工具学习曲线 
- ✅ 使用 Label Studio 开源数据标注平台，支持文档、问题和高亮标注 
- ✅ 使用 GPT-4 / Claude 等大模型，先帮你生成初步 QA 对，人工审核后再用 
- ✅ 使用 Haystack / LlamaIndex，通过检索获取支持文档。通过半自动获取数据集方式，加速标注流程
- 

### 4.3 检索评估
介绍完如何构建黄金数据集后，下一步我们进入到 RAG 评估流程的第三个环节——检索评估。 

检索评估的目标就是评估根据查询从**知识库**检索结果整个过程是否成功。 

注意我这里说的是“知识库”，而非向量库。也就是说，无论数据库、搜索引擎、文件系统、向量库、网络还是任何以存储数据的地方，都属于知识库的范畴。RAG 是检索增强生成，它并没有强制知识库一定是向量库，这里大家要注意，千万不要把路走窄了。 

#### 4.3.1 检索评估准召率定义 
做 RAG 这么久，我认为检索评估是全评估流程里，最困难的环节，也是最有技术挑战的环节。 

客观来说，召回率、准确率、命中率、MRR（平均倒数排名）、NDCG（归一化折扣累积增益）等指标非常重要。讲起这些指标以及背后的数学公式，会让我充满成就感，毕竟直击底层逻辑，最接近真理。 

但我相信，如果我真这么做了，肯定会劝退大多数人，大家肯定直接关闭我的视频走人。 

所以，深谙世故的我，会收起那份好为人师的善意，很多数学知识只会点到为止。

召回率recall

recall=（检索出的相关文档数量）/（所有实际存在的相关文档的数量）

### 4.3.3 检索评估指标示例
讲完准确率和召回率理论后，为了加深大家理解，我再举个实际例子。
先看下面的表格。注意，下表指标后面的k代表实际检索返回的文档个数。

| 指标名 | 中文解释 | 举例 | 用途 |
| --- | --- | --- | --- |
| Recall@k | 前k个检索结果中是否有相关文档 | 检索出前5个段落，其中1个是答案出处→Recall@5 = 1（命中） | 检查有没有返回 |
| Precision@k | 前k个检索结果中有多少是“真正相关”的？ | 检索出5个段落，只有2个真正有关→Precision@5 = 0.4 | 查得准不准 |
| Hit@k | 前k个结果中是否至少有一个命中正确文档？（命中=1，否则=0） | 若正确段落排在第4名→Hit@5 = 1 | 常用于粗略统计检索命中率 |
| MRR (Mean Reciprocal Rank) | 正确文档排名越靠前，分数越高 | 正确答案排在第1名得分1，第2名得分0.5，以此类推 | 衡量正确答案排序质量 |
| NDCG@k（选用） | 结合相关性和排名的加权评价 | 比MRR更细致，适用于有多个相关文档 | 大型系统中使用多一些 | 

列表了解完毕后，下面我们引入《喜剧之王》RAG系统。 

我们先问问题。 
问题（query）：“柳飘飘说‘玩玩而已’是在什么情境下？” 

该问题答案在知识库中是真实存在的。如下： 
柳飘飘：“你不会真…… 
场景描述：发生在两人一夜情后的清晨，柳飘飘装作冷漠回应尹天仇的认真。 

当我们把问题输入RAG系统后，系统在剧本知识库中进行检索，随后返回5个文档。4


|指标|结果|说明|
| ---- | ---- | ---- |
|Recall@5|= 1（命中）|前5个检索结果中有相关文档|
|Precision@5|1/5 = 0.2（只有1个相关）|前5个结果里真正相关的占比| 
|Hit@1|= 0（不在第一位）|前1个结果中无命中正确文档| 
|MRR|1/4 = 0.25（正确答案排第4）|因正确答案排第4，分数为1/4| 

由这个评估结果，我们得出的结论是：
- 检索器勉强命中了答案（Recall 还行） 
- 但质量很差，排序不好（Precision 和 MRR 很低） 

### 4.3.4 检索评估实战 
介绍完上面案例，其实我们就已经大体知道检索评估如何实现了。只需下面三步：
- ✅第一步：先构建检索黄金数据集。这一步我们在上面的章节已经准备好了。 
- ✅第二步：运行 RAG 检索系统，输入问题，返回前 k 个检索结果。 
- ✅第三步：评估检索效果。根据系统返回的文档和真实答案进行指标计算，获取检索评估指标值。 

### 4.3.5 检索评估最佳实践
| 问题 | 建议 |
| --- | --- |
| 2 检索不到正确信息？ | 检查 embedding 模型是否匹配知识库语料（试试 bge、qwen3-embedding、bce 等）。特别是知识含有大量缩写、行业黑话、专业术语，导致 embedding 语义捕捉效果差，建议微调嵌入模型。比如（HW：华为、半波、家庭作业、护网等） |
| 3 正确文档排得太后？ | 考虑加入 reranker（如 Cohere Rerank、bge-reranker） |
| 4 多个文档都包含结果？ | 用 NDCG 指标考虑“多个正确答案”的情况 |
| 5 检索内容太长或太短？ | 合理切片文档（如每段 200 - 500 字），提升可检索性 |

此外，关于 RAG 准确率和召回率的权衡之术（Trade-off），这里面的水既深又混，我也拿出我多年的心得，供大家批评指正。 

召回率和准确率通常存在此消彼长的关系：
- 为了提高召回率，也就是尽可能找到更多相关文档，常常需要放宽检索条件（如降低向量相似度阈值、扩大检索窗口等），但这么做往往会导致检索出更多不相关文档，从而降低准确率。 
- 为了提高准确率，也就是只返回最相关的文档，往往需要收紧检索条件（如提高向量相似度阈值、减少返回数量等），但这么做往往会……（内容显示不全 ） 

理想情况是既要又要，但在实际情况很难同时达到最优，需要根据具体业务场景进行权衡。 
- 比如你面对的是高完整性的场景（如问答、事实核查）：可能更倾向于优先保证高召回率，此时召回率就是北极星指标。确保关键信息不遗漏，然后依赖 LLM 鲁棒性去处理一些噪音，或通过后续 Reranking 提升准确率。


- 比如你面对的是高精度的场景（如法律、金融、代码补全）：可能更倾向于优先保证高准确率，确保提供给 LLM 的信息高度相关，避免被无关信息带偏。 

# 实操中，RAG系统提升两者的常见策略
### 提高召回率方法
- **优化分块策略（Chunking）**：尝试重叠分块、按语义/句子分块、多粒度分块等，避免关键信息被切分在边界丢失。 
- **改进检索器（Retriever）**：尝试不同的嵌入模型（Embedding Model）、微调嵌入模型使其更贴合下游任务、结合关键词检索（如 BM25）与向量检索，也就是混合检索（Hybrid Search）。 
- **扩大检索范围**：适当增加 `top-k`（返回文档数量），降低相似度阈值。 
- **查询扩展/改写（Query Expansion/Rewriting）**：让原始查询更丰富或更清晰。 

### 提升准确率
- **重排序（Reranking）**：这是最常用的有效手段！初步检索（如向量检索返回 `top-k`）后，使用一个更强大但计算成本更高的模型（如 Cross-Encoder）对候选文档进行精细相关性排序，只保留最相关 `top-n` 个给 LLM。 
- **优化检索器**：同样可以尝试更好的嵌入模型或微调，使其区分相关/不相关文档的能力更强。 
- **收紧检索范围**：提高相似度阈值，减少 `top-k`。 
- **元数据过滤**：如果文档有可用元数据（如来源、日期、类型），利用其进行预过滤。 
- **优化分块策略**：确保分块本身具有较好的语义完整性，避免包含过多无关内容。 

当然，相信看视频的朋友藏龙卧虎，如果有更多实战经验和解决思路，欢迎评论区和弹幕区留下足迹。 

### 第二段内容（含补充部分 ）
#### 提升召回率方法
- **优化分块策略（Chunking）**：尝试重叠分块、按语义/句子分块、多粒度分块等，避免关键信息被切分在边界丢失。 
- **改进检索器（Retriever）**：尝试不同的嵌入模型（Embedding Model）、微调嵌入模型使其更贴合下游任务、结合关键词检索（如 BM25）与向量检索，也就是混合检索（Hybrid Search）。 
- **扩大检索范围**：适当增加 `top-k`（返回文档数量），降低相似度阈值。 
- **查询扩展改写（Query Expansion/Rewriting）**：让原始查询更丰富或更清晰。 

#### 提升准确率
- **重排序（Reranking）**：这是最常用的有效手段！初步检索（如向量检索返回 `top-k`）后，使用一个更强大但计算成本更高的模型（如 Cross-Encoder）对候选文档进行精细相关性排序，只保留最相关 `top-n` 个给 LLM。 
- **优化检索器**：同样可以尝试更好的嵌入模型或微调，使其区分相关/不相关文档的能力更强。 
- **收紧检索范围**：提高相似度阈值，减少 `top-k`。 
- **元数据过滤**：如果文档有可用元数据（如来源、日期、类型），利用其进行预过滤。 
- **优化分块策略**：确保分块本身具有较好的语义完整性，避免包含过多无关内容。 

当然，相信看视频的朋友藏龙卧虎，如果有更多实战经验和解决思路，欢迎评论区和弹幕区留下足迹。 

### 4.4 生成评估


上面我们花了大篇幅介绍检索评估，其实总结下来就下面三句话：


1. 查得准吗？（Precision）

2. 查到了吗？（Recall / Hit） 

3. 查询结果靠前吗？（MRR） 


我们要知道检索评估是 RAG 成败的关键一步。如果你查都查不到，后面生成也是“巧妇难为无米之炊”。 

#### 4.4.1 生成评估定义 
如果说检索评估关注的是检索过程，那么生成评估关注的就是拿到检索的文档进行生成，然后判断生成答案是否正确、有用和忠实。 

#### 4.4.2 生成评估目标 
生成评估目标总结下来有以下几个，分别是：
- **准确性（Correctness）**：也就是说生成结果对不对，有没有基于事实，有没有答偏？ 
- **忠实性（Faithfulness / Groundedness）**：有没有按照知识库回答？有没有胡编乱造？ 
- **表达好（Fluency / Clarity）**：语言是否流畅，是否容易理解？ 
- **覆盖完整（Completeness）**：有没有答全？有没有遗漏关键信息？ 

### 4.4.3 生成评估示例
为了加深大家对生成评估的理解，我还是拿《喜剧之王》举例。  

还是先问问题：  
问题（query）：“柳飘飘对尹天仇说‘玩玩而已’是什么意思？”  

我们期待理想的生成结果应该是：  
“这句话出现在一夜情之后，柳飘飘对尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’她表面上显得冷漠，其实是在掩饰内心的脆弱和情感防御。”  

如果大模型最终能生成这样的结果，我就觉得非常完美。因为答案忠实剧本、语气自然而且还有解释。  

但如果最终生成的结果是下面几个，那就很糟糕了。  
- “柳飘飘真心喜欢尹天仇，这句话表达了爱意。” —— 这样的答复明显有点鬼畜了  
- “玩玩而已这句话是尹天仇对柳飘飘说的。” —— 这样的答复明显事实错误  
- “她说‘玩玩而已’，这是经典的周星驰搞笑台词。” —— 这样的答复明显是胡扯  


### 4.4.4 生成评估实战  
介绍完生成评估理论后，下一步我们进入生成评估实战。

### 生成评估方法
我们先谈谈生成评估到底都有哪些方法。生成评估常用以下三种方法：  

#### 1. 机器评估指标  
设置数学指标，然后机器计算，这些指标分为三个维度，如下所示：  

| 指标 | 含义 | 特点 |  
| ---- | ---- | ---- |  
| BLEU / ROUGE / METEOR | 基于统计方法（n-gram）计算生成的回答和“参考答案”有多少词重合，也就是词匹配 | 快，但偏字面，不够智能 |  
| BERTScore | 基于上下文 Embedding 看语义相似度，非词匹配 | 更准确，常用 |  
| QA-based eval | 使用另一个训练好的问答（QA）模型（通常称为“评判模型”或“评估器模型”）来自动评估生成结果 | 适合问答系统 |  

这种评估方法类似检索评估指标，本质是依靠统计学、向量计算等数学方法计算生成内容和期望内容的匹配度或相似度。该方法可以适合大规模快速评估，但缺点也显而易见，无法判断“是否胡编”，也非常依赖参考答案的质量。  


#### 2. 用大语言模型做评审（LLM-as-a-Judge）  
可以通过 GPT-4、Claude 等大模型对生成结果打分，只需设计好提示词即可，比如：  

**代码块**  
```
Prompt:
你是一个评估专家，请判断下面的回答内容是否忠于提供的参考资料，是否完整、准确。  
参考资料：……  
回答内容：……  
请从准确性、忠实性、表述清晰度三方面分别评分。  
```  

该方法优点是方便、经济、智能、理解语义。缺点在于评审结果不稳定或不一致（当然也有解决方案，比如：让多个模型评审，然后取平均值）。  


#### 3. 人工评估  
最后一种方式就是人工评估了，这是成本最高的方式。虽然最贵，但也是最权威的。操作方式如下：  

| 维度 | 打分方式示例 |  
| ---- | ---- |  
| 正确性 | 回答是否和真实情况一致？（1-5分） |  
| 忠实性 | 回答是否从检索内容中推导出？（1-5分） |  
| 表达性 | 有没有语病？读起来顺不顺？（1-5分） |  
| 完整性 | 是否漏答、简略、偏题？（1-5分） |  

但人工评估方式，……（内容显示不全 ）  

结合上面三种生成评估方法，实际经常采用的策略是打组合拳，即：  
- 小数据集采用人工评估；  
- 大数据集先用模型代评，然后再抽样人工复查。  


### 4.4.4.2 生成评估流程  
介绍完生成评估的方法后，下一步我们总结下实施流程：  
- ✅第一步：还是构建黄金数据集。  
- ✅第二步：跑一轮 RAG，拿到生成结果。  
- ✅第三步：用以下方法评估生成答案：  
  - 语义相似度（BERTScore）  
  - 忠实性判断（用 GPT-4、Gemini、Grok、Claude 做 Groundedness 评分）  
  - 人工抽样查看  
- ✅第四步：根据评估结果记录错误类型：  
  - 是否胡说八道（幻觉）  
  - 是否漏答  
  - 是否答非所问  
  - 是否逻辑混乱  


### 4.4.4.3 生成评估最佳实践  
有关生成评估最佳实践，我也列了一张表，供大家参考。

### 4.4.4.3 生成评估最佳实践
有关生成评估最佳实践，我也列了一张表，供大家参考。

| 问题 | 原因 | 解决方案 |
| ---- | ---- | ---- |
| 编造信息 | 检索内容没覆盖到，或模型太自由 | 限制 Prompt：只允许基于文档回答 |
| 回答太短 | Prompt 不完整 | 明确要求“解释原因+引用原文” |
| 幻觉 | 检索+Prompt 双问题 | 优化问题匹配并进行问题指引 |  

从上面内容以及模型原理其实可以得出结论，如果想提升生成效果，很大程度上依赖于把提示词写好。  


### 4.5 端到端评估（E2E Evaluation）  
检索评估和生成评估介绍完毕后，我们再介绍下端到端评估。  


#### 4.5.1 端到端评估定义  
我们都清楚，在 RAG 系统中：  
- 检索器（Retriever）：用来检索相关文档  
- 生成器（Generator）：基于检索的文档生成回答  
- 最终输出：是呈现给用户的回答  

所以，端到端评估（End-to-End Evaluation）就是从用户视角整体评估整个流程表现。也就是说作为最终用户视角，他根本就不关心：  
- 第一层，有没有查准  
- 第二层，有没有答对  

而它只关心：  
“关于我提出的问题，你给我的回答够不够好？”  

以及根据最终结果，他的态度，那便是：  
这系统值不值得我买单  


#### 4.5.2 端到端评估目标  
上面我已经非常详细介绍过生成评估，这为介绍端到端评估打下非常好的基础，下面我列了一张表：  

| 维度 | 解释 | 举例问题 |  
| ---- | ---- | ---- |  
| ✅正确性（Correctness） | 回答是否客观真实？有没有事实错误？ | 你是不是在胡编乱造？ |  
| ✅忠实性（Faithfulness） | 回答是否真的基于你检索到的内容？ | 你是不是脱离材料自由发挥了？ |  
| ✅有用性（Helpfulness） | 回答是否对用户有价值？ | 这回答对我有没有用？ |  
| ✅完整性（Completeness） | 有没有漏答关键点？ | 说全了吗？有没有跳过重要信息？ |  
| ✅可读性（Fluency / Clarity） | 回答流不流畅，好不好读？ | 像不像一个正常人写的？ |  

从列表看，其实端到端的评估目标跟生成评估的目标非常类似。  

但必须明确的一点是，E2E 是最高视角，更贴近“用户体验”和“产品价值”。  

而且 E2E 会主观，因为主观，所以在跟客户交流的过程中，更多需要沟通技巧，做好情绪价值。  

这是你距离成功的最后一公里，但这一步却异常难走。  


#### 4.5.3 端到端评估示例  
为了加深大家对端到端评估的理解，继续拿《喜剧之王》做示例。  

还是先问问题：  
“柳飘飘说‘玩玩而已’是什么意思？”  

RAG 系统生成回答：

### 4.5.4.1 端到端评估方法
常用端到端评估方法，一般也有三种方式：  

#### 1. 人工打分  
适合该方式的场景是高质量、小批量的数据集，人工可以根据黄金数据样本给出分数或评语  

| 评分维度 | 打分标准（1-5） | 示例 |
| ---- | ---- | ---- |
| 正确性 | 5 = 完全正确，1 = 完全错误 | “事实正确性是否可信？” |
| 忠实性 | 5 = 完全基于资料，1 = 明显幻觉 | “回答是否引用文档而不是编造？” |
| 有用性 | 5 = 非常有帮助，1 = 没有帮助 | “对用户有没有实际价值？” |
| 完整性 | 5 = 答全要点，1 = 缺失关键信息 | “有没有遗漏问题中关注的方面？” |
| 可读性 | 5 = 通顺流畅，1 = 混乱不通 | “是否像正常自然语言表达？” |  

技术手段我们可以通过 Excel、问卷、Notion 等方式组织业务专家或打标人员进行评分。  

人工端到端评估的好处是精准，缺点就是成本高。  


#### 2. 大语言模型评分（LLM-as-a-Judge）  
类似于生成评估的方式，你也可以用 GPT-4 / Claude 等模型帮你评估回答效果，前提是使用好提示词，比如下面我给的提示词样例：  

**代码块**  
```
你是一个问答系统评估专家。请从以下五个维度对回答进行 1-5 分打分：
1. 正确性
2. 忠实性（是否基于提供资料）
3. 有用性
4. 完整性
5. 可读性

问题：
回答：
可参考文档：
```  

当然，你也可以通过工具，比如：  
- **Ragas**：内置端到端评估模块，支持 correctness + groundedness + helpfulness  

基于 LLM 的端到端的评估优点是快、可扩展，缺点是不稳定，需抽样校验。  


#### 3. 用户反馈评分（线上系统）  
最后一种端到端评估方式就是系统上线后，通过收集用户「👍/👎」。这种方式是最自然的 E2E 评估。  

这种方式的优点是实用价值最高，但缺点在于不系统、偏主观、样本少时不稳定。  


### 4.5.4.2 端到端评估最佳实践  
关于端到端评估最佳实践，下面我也列了一张表，供大家参考。  

| 问题类型 | 解决方式 |
| ---- | ---- |
| 没有命中支持内容 | 提升检索 Recall，加入 reranker |
| 胡说八道 / 编故事 | Prompt 限制只基于文档生成 |
| 回答不完整 | Prompt 引导“逐点展开” |
| 语言怪异 / 风格不自然 | 模型 fine-tune 或 prompt 精调 |  

端到端评估跟生成评估很多时候可以合并做，区别在于端到端评估可将真实线上评估结果做数据回流。

# rag评估体系构建

```python
# RAG 评估全流程代码实战（基于《喜剧之王》示例）
from typing import List, Dict
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np
from difflib import SequenceMatcher

# --- 第一步：准备数据集 ---
# 示例黄金数据集（问题+答案+支持文档）
golden_data = [
    {
        "question": "《喜剧之王》中柳飘飘一夜情后对尹天仇说了什么？",
        "answer": "在电影《喜剧之王》中，一夜情之后，柳飘飘对尹天仇说：“你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。”",
        "supporting_docs": [
            "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’",
            "她刻意用轻松的语气掩饰自己的复杂情绪，试图拉开距离。"
        ]
    }
]

# --- 第二步：检索模拟（简单语义匹配） ---
# 初始化向量模型
encoder = SentenceTransformer('all-MiniLM-L6-v2')

# 模拟知识库（假设已被切分成文档片段，这里直接用 golden_data 里的 supporting_docs 构造演示 ）
document_store = [
    "柳飘飘第一次见尹天仇是在酒吧的后台。",
    "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’",
    "尹天仇梦想成为演员，坚持每天排练台词。",
    "她刻意用轻松的语气掩饰自己的复杂情绪，试图拉开距离。"
]

def retrieve(question: str, top_k=3) -> List[str]:
    """
    模拟检索过程：
    1. 对问题和文档库内容分别编码
    2. 计算语义相似度
    3. 返回相似度最高的 top_k 个文档片段
    """
    q_emb = encoder.encode([question])
    doc_embs = encoder.encode(document_store)
    scores = cosine_similarity(q_emb, doc_embs)[0]
    # 取相似度高的 top_k 个文档索引（降序排列后取前 top_k）
    top_indices = scores.argsort()[::-1][:top_k]
    return [document_store[i] for i in top_indices]

# --- 第三步：生成模拟（简单拼接，实际场景可替换为 LLM 调用 ） ---
def generate_answer(retrieved_docs: List[str]) -> str:
    """
    模拟生成过程：直接拼接检索到的文档片段作为回答
    （实际项目中，这里会调用大语言模型基于文档生成更自然的回答）
    """
    return "基于资料回答：" + " | ".join(retrieved_docs)

# --- 第四步：评估 ---
def evaluate_retrieval(gt_docs: List[str], retrieved_docs: List[str]) -> float:
    """
    评估检索效果：判断检索到的文档与真实支持文档是否匹配（相似度 > 0.8 视为命中）
    返回是否命中（1 表示命中，0 表示未命中 ）
    """
    for gt in gt_docs:
        for r in retrieved_docs:
            if SequenceMatcher(None, gt, r).ratio() > 0.8:
                return 1.0  # Hit@k
    return 0.0

def evaluate_generation(golden: str, generated: str) -> float:
    """
    评估生成效果：简单计算生成回答与真实答案的文本相似度
    （实际可结合更多维度，如忠实性、可读性等 ）
    """
    return SequenceMatcher(None, golden, generated).ratio()

# --- 第五步：运行完整流程 ---
if __name__ == "__main__":
    for sample in golden_data:
        question = sample["question"]
        gt_answer = sample["answer"]
        gt_docs = sample["supporting_docs"]
        
        print("\n=== 问题 ===")
        print(question)
        
        # 执行检索
        retrieved = retrieve(question)
        print("\n检索结果：")
        for d in retrieved:
            print("-", d)
        
        # 评估检索
        hit_score = evaluate_retrieval(gt_docs, retrieved)
        print(f"\n检索 Hit@k: {hit_score}")
        
        # 执行生成
        gen_answer = generate_answer(retrieved)
        print("\n生成答案：")
        print(gen_answer)
        
        # 评估生成
        gen_score = evaluate_generation(gt_answer, gen_answer)
        print(f"\n生成相似度得分：{round(gen_score, 3)}")
```

### 代码说明
1. **数据准备**：定义了 `golden_data`，包含问题、真实答案、支持文档，用于模拟实际业务中的“黄金数据集”。  
2. **检索模拟**：用 `SentenceTransformer` 做文本编码，通过余弦相似度计算实现简单语义检索，模拟 RAG 中的检索器（Retriever）。  
3. **生成模拟**：`generate_answer` 直接拼接检索结果演示生成过程，实际项目需替换为调用大语言模型（如 GPT、LLaMA 等）生成自然回答。  
4. **评估逻辑**：  
   - `evaluate_retrieval`：通过文本相似度判断检索结果是否命中真实支持文档，模拟检索评估（如 Hit@k 指标 ）。  
   - `evaluate_generation`：计算生成回答与真实答案的文本相似度，简单模拟生成评估（实际可扩展更多维度，如忠实性、可读性 ）。  
5. **端到端执行**：遍历 `golden_data`，依次执行“检索→检索评估→生成→生成评估”，输出每一步结果，完整演示 RAG 评估流程。  

可根据实际需求扩展，比如替换生成逻辑为真实大模型调用、优化评估指标（加入忠实性、可读性判断 ）等。


以下是提取的全部文字内容，按截图顺序整理：

### 第一组截图（环境与项目开发）
#### 5.1 设置环境  
首先，我们需要设置环境。  

##### 5.1.1 创建虚拟环境  
使用 conda create 指令创建一个虚拟环境 rag。创建虚拟环境的目的是为了避免不同 python 项目冲突。  

**代码块**  
```
conda create -n rag python=3.11
```  

##### 5.1.2 激活虚拟环境  
（截图中未完整显示，结合上下文推测是激活虚拟环境的指令，如 `conda activate rag`  ）  


#### 5.2 开发项目  
整个 RAG 评估全流程代码，包含以下模块：  
1. ✅ 数据准备（问题、答案、支持文档）  
2. 📡 检索模块（向量检索模拟）  
3. 📝 生成模块（拼接文档模拟）  
4. 📊 评估模块（检索 hit 和生成相似度）  
5. 🚀 端到端执行与输出展示  

整个代码如下：  

**代码块（Python 示例）**  
```python
# RAG 评估全流程代码实战（基于《喜剧之王》示例）
from typing import List, Dict

# --- 第一步：准备数据集 ---
# 示例黄金数据集（问题+答案+支持文档）
golden_data = [
    {
        "question": "《喜剧之王》中柳飘飘一夜情后对尹天仇说了什么？",
        "answer": "在电影《喜剧之王》中，一夜情之后，柳飘飘对尹天仇说：“你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。”",
        "supporting_docs": [
            "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’",
            "她刻意用轻松的语气掩饰自己的复杂情绪，试图拉开距离。"
        ]
    }
]

# --- 第二步：检索模拟（简单语义匹配） ---
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np

# 初始化向量模型
encoder = SentenceTransformer('all-MiniLM-L6-v2')

# 模拟知识库（假设已被切分成文档片段）
document_store = [
    "柳飘飘第一次见尹天仇是在酒吧的后台。",
    "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’",
    "尹天仇梦想成为演员，坚持每天排练台词。",
    "她刻意用轻松的语气掩饰自己的复杂情绪，试图拉开距离。"
]

def retrieve(question: str, top_k=3) -> List[str]:
    q_emb = encoder.encode([question])
    doc_embs = encoder.encode(document_store)
    scores = cosine_similarity(q_emb, doc_embs)[0]
    top_indices = scores.argsort()[::-1][:top_k]
    return [document_store[i] for i in top_indices]

# --- 第三步：生成模拟（简单拼接） ---
def generate_answer(retrieved_docs: List[str]) -> str:
    return "基于资料回答：" + " | ".join(retrieved_docs)

# --- 第四步：评估 ---
from difflib import SequenceMatcher

def evaluate_retrieval(gt_docs: List[str], retrieved_docs: List[str]) -> float:
    hit = 0
    for gt in gt_docs:
        for r in retrieved_docs:
            if SequenceMatcher(None, gt, r).ratio() > 0.8:
                hit = 1
                break
        if hit:
            break
    return hit  # Hit@k

def evaluate_generation(golden: str, generated: str) -> float:
    return SequenceMatcher(None, golden, generated).ratio()  # 粗略相似度评估

# --- 第五步：运行完整流程 ---
for sample in golden_data:
    question = sample["question"]
    gt_answer = sample["answer"]
    gt_docs = sample["supporting_docs"]
    
    print("\n=== 问题 ===")
    print(question)
    
    retrieved = retrieve(question)
    print("\n检索结果：")
    for d in retrieved:
        print("-", d)
    
    hit_score = evaluate_retrieval(gt_docs, retrieved)
    print(f"\n检索 Hit@k: {hit_score}")
    
    gen_answer = generate_answer(retrieved)
    print("\n生成答案：")
    print(gen_answer)
    
    gen_score = evaluate_generation(gt_answer, gen_answer)
    print(f"\n生成相似度得分：{round(gen_score, 3)}")
```  

若还有其他零散文字（如操作提示、说明语），本质已包含在上述流程和代码注释里，可结合实际截图核对。核心是围绕 **RAG 评估全流程** 的代码实现，从数据准备、检索/生成模拟，到评估与端到端执行 。




# 评估指标
检索器评估指标
## 准确率：
context precision

t he signal to moise ratio of retrieved context

评估查询结果准不准确
## 召回率
系统检索出相关的文档，所有实际相关的文档

context recall

can it retrieve all the relevant information required to answer the question

评估查询结果全不全

## 生成器评估指标

## 忠实度

指的是生成结果是基于检索的结果，事实准确的，评估生成内容是否是谎言

faithfulness

how factually acurate is the generated answer
## 相关性
answer relevancy

how relevant is the generated answer to the question




