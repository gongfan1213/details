https://abdullin.com/ilya/how-to-build-best-rag/

https://abdullin.com/ilya/how-to-build-best-rag/


# 作者在企业RAG挑战赛中遇到的困难及解决方案
## 一、PDF解析相关困难及解决
1. **困难**
    - PDF解析存在诸多细微难题，如表格结构保留、关键格式元素（标题、项目符号列表等）保留、多列文本识别、图表及页眉页脚处理等。
    - 还遇到特殊问题，例如大表格有时会旋转90度导致解析器生成乱码；部分文档存在字体编码问题，视觉上文本正常，但复制或解析时会得到无意义字符，经研究发现是凯撒密码，每个单词的ASCII偏移量不同。
2. **解决方法**
    - **选择合适解析器**：试用了约二十几种PDF解析器后，选择了表现最佳的Docling，其由竞赛主办方之一IBM开发。
    - **定制解析器**：彻底检查Docling库的源代码，重写部分方法以满足需求，解析后得到包含所有必要元数据的JSON，再利用该JSON构建格式修正的Markdown文档，支持表格结构转换为MD和HTML格式。
    - **提升解析速度**：利用GPU加速解析，为竞赛租用配备4090 GPU的虚拟机（每小时70美分），使100份文档的解析耗时约40分钟，达到极高的解析速度。
    - **处理特殊编码文档**：对于存在凯撒密码的文档，尝试解码，虽修复后仍有许多噪声，但通过全量光学字符识别（OCR）处理这些文档。

## 二、文本处理与表格相关困难及解决
1. **困难**
    - 从PDF解析出的部分文本可能存在错误，包含特定语法，降低可读性和表意性。
    - 在大型表格中，指标名称（水平表头）与垂直表头相距过远（有时有1500个不相关标记），削弱语义连贯性，降低文本块在向量搜索中的相关性，且大语言模型难以匹配指标名称与标题，可能返回错误值。
2. **解决方法**
    - **文本清洗**：使用一组十几个正则表达式处理解析错误的文本，去除无关噪声。
    - **表格序列化**：将大表转换为一组小的、上下文无关的字符串，经过大量实验，发现将表格以HTML格式提供给LLM效果更好，能更好地处理合并单元格、子标题等结构复杂的表格。不过，最终获胜方案未使用该方法，因Docling对PDF表格的解析已足够好，额外添加文本反而降低信噪比。

## 三、检索与排序相关困难及解决
1. **困难**
    - 混合搜索（语义向量搜索与传统关键词搜索结合）在简单实现时，不仅没有提高检索质量，反而降低了检索质量。
    - 交叉编码器重排序虽能给出更精确的相似性分数，但速度较慢，且通过API可用的交叉编码器重排序模型较少，OpenAI等大型提供商未提供，管理另一个API余额较麻烦。
2. **解决方法**
    - 放弃混合搜索和交叉编码器重排序方法。
    - 采用LLM重排序：将文本和问题传给LLM，让其对文本与问题的相关性从0到1打分，并通过结构化输出返回推理过程和相关性分数。优化流程，一次请求同时发送三个页面，让LLM同时返回三个分数，提高速度、降低成本并略微提高评分一致性。通过加权平均（向量权重0.3，LLM权重0.7）计算修正的相关性分数，该方法成本低（每个问题不到1美分），且在质量、速度和成本间达到良好平衡。

## 四、查询处理与生成相关困难及解决
1. **困难**
    - 现实场景中，查询路由到数据库比竞赛中的受控环境更复杂，需额外的预处理任务。
    - 大语言模型的认知能力有限，给予的规则越多，其忽略规则的可能性越大，难以同时考虑所有细微差别而不犯错。
    - 竞赛中存在比较多家公司指标的复合问题，这类问题不符合简单查询的范式，需要额外步骤才能回答。
    - 当报告中缺乏确切请求的指标时，上下文可能包含相似但不完全相同的信息，模型可能会扭曲提供的数据以适应请求的指标，因模型倾向于提供答案而不是承认无知。
    - 并非所有LLM都支持结构化输出，即使在提示词中提供输出 schema，部分答案也难免偏离 schema，尤其是较小的模型，约有一半时间不符合要求。
2. **解决方法**
    - **查询路由到数据库**：在竞赛中，利用问题中明确出现的公司名称（竞赛提供所有公司名称列表），通过`re.search()`从问题中提取公司名称，匹配到相应数据库，将搜索空间缩小100倍。在现实场景中，可通过标记数据库或使用LLM从问题中提取实体以匹配数据库。
    - **查询路由到提示词**：由于问题的预期响应类型明确，针对不同答案类型（int/float、bool、str、list[str]）编写四种提示词变体，通过简单的`if else`选择正确的提示词，减少每个查询的规则数量。
    - **处理复合查询**：将初始比较问题传给LLM，让其创建针对每家公司的简单子问题，分别通过标准流程处理这些子问题，收集每家公司的答案后，将其纳入上下文以回答原始问题。
    - **减少幻觉**：采用思维链（CoT）技术，让模型在提供最终响应前“自言自语”，生成一系列中间推理步骤，明确指导模型推理方式，解释推理步骤和目标并提供示例，尤其让模型专注于评估问题与上下文之间指标的兼容性，显著减少幻觉。
    - **结构化输出处理**：对于不支持结构化输出的模型，将输出 schema 直接放在提示词中，编写 fallback 方法，通过`schema.model_validate(answer)`验证模型响应是否符合 schema，若验证失败，将响应返回给LLM，提示其符合 schema，使即使是8b模型的 schema 合规性也达到100%。

## 五、系统速度与配置相关困难及解决
1. **困难**
    - 竞赛最初规则要求系统在10分钟内回答所有100个问题才能有资格获得奖金，对系统速度要求较高。
    - 需要确定各种功能对系统的实际影响，以调整超参数，优化系统性能。
2. **解决方法**
    - **提升系统速度**：充分利用OpenAI的Tokens Per Minute速率限制，估计每个问题的令牌消耗，将问题分批处理（每批25个），使系统在2分钟内完成所有100个问题的回答，即使在规则放宽后，仍保持了高效的处理能力。
    - **系统配置优化**：将所有关键功能设为可配置，通过配置字段（如`use_serialized_tables`、`llm_reranking`等）测量其实际影响并微调超参数，经测试发现表格序列化不仅没有改善系统，反而略微降低了其有效性，最终确定了合适的系统配置，使系统在开源和专有模型上均表现出色，Llama 3.3 70b仅略落后于OpenAI的o3-mini，甚至小模型Llama 8b也超过了80%的参与者。
  

# Ilya Rice：企业RAG挑战赛夺冠经验总结
## 一、赛事简介
- **任务**：基于100份随机公司年度报告（PDF格式，每份最长1000页），在2.5小时内解析并建立数据库，随后快速回答100个随机生成的问题。
- **答案要求**：需有明确答案（如是/否、公司名称、数值指标等），且每个答案需包含提供证据的页面引用，确保答案非虚构。

## 二、获胜系统架构
- 除基本步骤外，融入了两个路由器和大语言模型（LLM）重排序机制。

## 三、系统构建步骤
### 1. 解析（Parsing）
- **PDF解析难点**：需解决表格结构保留、关键格式元素保留、多列文本识别、图表及特殊格式处理等问题，还会遇到表格旋转、字体编码异常等特殊情况。
- **解析器选择**：试用约20多种解析器后，选择Docling，其表现最佳，且由竞赛主办方之一IBM开发。
- **解析器定制**：重写Docling部分方法，解析后得到含必要元数据的JSON，再构建格式修正的Markdown文档，支持表格结构转换为MD和HTML格式。
- **提速措施**：利用GPU加速解析，租用配备4090 GPU的虚拟机，100份文档解析耗时约40分钟。
- **文本清洗与表格准备**：用正则表达式处理解析错误的文本；针对大型表格垂直与水平表头距离过远的问题，采用表序列化方法，将大表转为小的、上下文无关的字符串，但最终获胜方案未使用该方法。

### 2. 摄取（Ingestion）
- **术语定义**：“document”指日常意义上的文档（如报告等），数据库中存储的文本片段称为“chunk”。
- **分块（Chunking）**：将每页文本分成300令牌（约15个句子）的块，使用带自定义MD字典的递归分割器，添加50令牌的文本重叠，每个块的元数据存储其ID和父页面编号。
- **向量化（Vectorization）**：为每份文档创建单独的向量数据库，使用FAISS工具，采用IndexFlatIP方法创建数据库，通过内积（IP）计算相关性分数，使用text-embedding-3-large将块和查询嵌入为向量表示。

### 3. 检索（Retrieval）
- **混合搜索（vDB + BM25）**：结合语义向量搜索与传统关键词搜索，但在该赛事中效果不佳，未深入探索。
- **交叉编码器重排序**：能给出更精确的相似性分数，但速度较慢，且因API可用模型少而放弃，推荐Jina Reranker。
- **LLM重排序**：将文本和问题传给LLM，让其对相关性评分（0-1分），以结构化输出形式返回评分及推理过程，通过加权平均（向量权重0.3，LLM权重0.7）计算修正的相关性分数，成本低且效果好。
- **父页面检索**：找到相关块后，以其为指针获取完整页面内容纳入上下文。
- **检索步骤**：对查询向量化，找到30个相关块，通过块元数据提取页面（去重），经LLM重排序后调整页面相关性分数，返回前10页并合并为字符串。

### 4. 增强（Augmentation）
- **提示词存储结构**：在专用的prompts.py文件中存储提示词，将其分为核心系统指令、定义LLM响应格式的Pydantic模式、示例问答对、插入上下文和查询的模板等逻辑块，通过函数按需组合成最终提示词配置，解决指令重复更新问题，简化长提示词处理。

### 5. 生成（Generation）
- **查询路由到数据库**：利用问题中明确出现的公司名称，通过正则匹配将查询路由到对应公司的数据库，缩小搜索范围。
- **查询路由到提示词**：根据问题预期答案类型（int/float、bool、str等），提供相应的指令集，编写4种提示词变体，通过简单的条件判断选择正确的提示词。
- **复合查询路由**：对于比较类等复杂查询，先让LLM生成针对各公司的子问题，分别处理子问题后，结合答案回答原始问题。
- **思维链（Chain of Thoughts）**：让模型在给出最终答案前生成中间推理步骤，通过明确指导模型推理方式、解释推理步骤和目标并提供示例，提升答案质量，减少幻觉。
- **结构化输出（Structured Outputs）**：强制模型以严格定义的格式响应，通常通过API的单独参数（如Pydantic或JSON模式）实现，确保返回内容符合规范。
- **CoT与SO结合（CoT SO）**：模型有专门的推理字段和单独的最终答案字段，主 schema 包含step_by_step_analysis、reasoning_summary、relevant_pages、final_answer四个字段。
- **SO重解析器（SO Reparser）**：对于不支持结构化输出的模型，将输出模式直接放在提示词中，若模型响应不符合模式，通过验证后将响应返回给LLM，提示其符合模式，确保 schema 合规性达100%。
- **单样本提示（One-shot Prompts）**：在每个提示词中添加“问题→答案”对，以结构化输出定义的JSON格式编写答案，可展示推理过程、阐明特殊情况处理方式、说明JSON结构。
- **指令优化（Instruction Refinement）**：深入研究问题生成器和响应要求，明确用户问题隐含意义，定义“解释自由度阈值”，解决歧义问题，通过生成验证集并手动分析，将澄清内容纳入提示词作为指令集。
- **系统速度**：为满足在10分钟内回答100个问题的要求，利用OpenAI的Tokens Per Minute速率限制，按问题的令牌消耗估算，以25个为一批处理问题，100个问题仅用2分钟完成。
- **系统质量**：通过配置关键功能，测试不同配置并微调超参数，发现表格序列化对系统有轻微负面影响，最终系统在开源和专有模型上均表现出色。

## 四、总结
- 夺冠关键在于采用系统方法，合理组合和微调各种方法，深入研究任务细节，高质量的解析、高效的检索、智能的路由，尤其是LLM重排序和精心设计的提示词，使系统即使使用小型模型也能取得优异成绩。
- 核心收获：RAG的魔力在于细节，对任务理解越透彻，对每个 pipeline 组件微调越精确，即使是最简单的技术也能带来更大收益。
- 相关资源：系统代码已开源，包含部署和运行 pipeline 各阶段的说明。
